{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import Tensor\n",
                "from transformer_lens import HookedTransformer\n",
                "from transformer_lens.hook_points import HookPoint\n",
                "from jaxtyping import Float, Int\n",
                "import requests\n",
                "import functools\n",
                "from pathlib import Path\n",
                "import importlib\n",
                "\n",
                "import nest_asyncio\n",
                "\n",
                "nest_asyncio.apply()\n",
                "from n2g import NeuronModel, FeatureModel, Tokenizer\n",
                "import mechint\n",
                "\n",
                "importlib.reload(mechint)\n",
                "from mechint import n2g"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model gpt2-small into HookedTransformer\n"
                    ]
                }
            ],
            "source": [
                "model_large = HookedTransformer.from_pretrained(\"gpt2-small\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c32a4fc993d44b15bee9f94599372795",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/10 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "'the cat is smwing me â™¥\\n\\ncat swallowed girl drinkers'"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_large.generate(\"the cat is sm\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([19.7922, 19.6165, 19.4624, 18.2938, 17.9073], device='cuda:0',\n",
                        "       grad_fn=<TopkBackward0>)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "['eared', 'itten', 'elly', 'ot', 'okin']"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sample = model_large.to_tokens(\"the cat is sm\")\n",
                "top_logits, top_tokens = model_large.run_with_hooks(sample)[0, -1, :].topk(k=5, dim=-1)\n",
                "print(top_logits)\n",
                "model_large.to_str_tokens(top_tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "r = requests.get(\"https://deepdecipher.org/api/gpt2-small/neuron2graph-search?query=activating:sm\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "layers = [[]] * 12\n",
                "for index in r.json()[\"data\"]:\n",
                "    layer = index[\"layer\"]\n",
                "    neuron = index[\"neuron\"]\n",
                "    layers[layer].append(neuron)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([20.9554, 20.9068, 20.5438, 19.5182, 18.8950], device='cuda:0',\n",
                        "       grad_fn=<TopkBackward0>)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "['eared', 'elly', 'itten', 'ot', 'okin']"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "hooks = []\n",
                "\n",
                "\n",
                "def hook_fn(\n",
                "    indices: Int[Tensor, \" _\"], activation: Float[Tensor, \"batch context neurons_per_layer\"], hook: HookPoint\n",
                ") -> None:\n",
                "    activation[:, -1, indices] = 0.0\n",
                "\n",
                "\n",
                "for layer_index, neurons in enumerate(layers):\n",
                "    indices = torch.tensor(neurons)\n",
                "    hook = functools.partial(hook_fn, indices)\n",
                "    hooks.append((f\"blocks.{layer_index}.mlp.hook_post\", hook))\n",
                "\n",
                "\n",
                "top_logits, top_tokens = model_large.run_with_hooks(sample, fwd_hooks=hooks)[0, -1, :].topk(k=5, dim=-1)\n",
                "print(top_logits)\n",
                "model_large.to_str_tokens(top_tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4d8657842ab84f9fa2500a3bf67c55d8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/10 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "'the cat is smothering my ear and my right ear.yes'"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_large.generate(\"the cat is sm\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model gelu-1l into HookedTransformer\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "69bf027e0fb14e3f8ef82b309291259a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/10 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "'the cat is smitten by several sadities. She has not said'"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_gelu = HookedTransformer.from_pretrained(\"gelu-1l\")\n",
                "model_gelu.generate(\"the cat is sm\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dict_keys([',', 'and', 'user', '0', '3', '.', 'glad', 'name', 'queries', 'just', 'humans', 'suggests', 'related', 'ning', 'sm', 'iles', 'family'])\n",
                        "dict_keys(['system', 'arlington', 'ole', 'sm', 'dent', 'vector', 'OW', 'th', 'igator'])\n"
                    ]
                }
            ],
            "source": [
                "n2g_path = Path(\"outputs/gelu-1l-sae-n2g\")\n",
                "\n",
                "\n",
                "def activates_on(n2g_model: NeuronModel, token: str | list[str]) -> bool:\n",
                "    if isinstance(token, str):\n",
                "        token = [token]\n",
                "    activating = n2g_model.trie_root.children.keys()\n",
                "    return any(t in activating for t in token)\n",
                "\n",
                "\n",
                "sm_n2g_models = [model for model in n2g.iter_models(n2g_path, range(2048, 4096)) if activates_on(model, [\"sm\", \" sm\"])]\n",
                "\n",
                "len(sm_n2g_models)\n",
                "for n2g_model in sm_n2g_models:\n",
                "    print(n2g_model.trie_root.children.keys())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AssertionError",
                    "evalue": "given string partly should be tokenized to exactly one token",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m root \u001b[38;5;241m=\u001b[39m n2g_model\u001b[38;5;241m.\u001b[39mtrie_root\n\u001b[0;32m     11\u001b[0m gelu_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(model_gelu)\n\u001b[1;32m---> 12\u001b[0m rs_model \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgelu_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn2g_model\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:93\u001b[0m, in \u001b[0;36mFeatureModel.from_model\u001b[1;34m(tokenizer, model)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_model\u001b[39m(tokenizer: Tokenizer, model: NeuronModel) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     92\u001b[0m     trie_root \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrie_root\n\u001b[1;32m---> 93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrie_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FeatureModel(RustFeatureModel\u001b[38;5;241m.\u001b[39mfrom_nodes(nodes), tokenizer)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_model\u001b[39m(tokenizer: Tokenizer, model: NeuronModel) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     92\u001b[0m     trie_root \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrie_root\n\u001b[0;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 94\u001b[0m         (_str_token_to_token(tokenizer, str_token), \u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m trie_root\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     96\u001b[0m     ]\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FeatureModel(RustFeatureModel\u001b[38;5;241m.\u001b[39mfrom_nodes(nodes), tokenizer)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:66\u001b[0m, in \u001b[0;36m_node_rust_to_py\u001b[1;34m(tokenizer, node)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[1;32m---> 66\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneuron_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_TOKEN\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[0;32m     66\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 67\u001b[0m         (_str_token_to_pattern_token(tokenizer, str_token), \u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m!=\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mEND_TOKEN\n\u001b[0;32m     70\u001b[0m     ]\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:66\u001b[0m, in \u001b[0;36m_node_rust_to_py\u001b[1;34m(tokenizer, node)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[1;32m---> 66\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneuron_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_TOKEN\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[0;32m     66\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 67\u001b[0m         (_str_token_to_pattern_token(tokenizer, str_token), \u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m!=\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mEND_TOKEN\n\u001b[0;32m     70\u001b[0m     ]\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
                        "    \u001b[1;31m[... skipping similar frames: <listcomp> at line 67 (2 times), _node_rust_to_py at line 66 (2 times)]\u001b[0m\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:66\u001b[0m, in \u001b[0;36m_node_rust_to_py\u001b[1;34m(tokenizer, node)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[1;32m---> 66\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneuron_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_TOKEN\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[0;32m     66\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 67\u001b[0m         (\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m, _node_rust_to_py(tokenizer, child))\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m!=\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mEND_TOKEN\n\u001b[0;32m     70\u001b[0m     ]\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:55\u001b[0m, in \u001b[0;36m_str_token_to_pattern_token\u001b[1;34m(tokenizer, str_token)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m==\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mIGNORE_TOKEN:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PatternToken\u001b[38;5;241m.\u001b[39mignore()\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PatternToken\u001b[38;5;241m.\u001b[39mregular(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\tokenizer.py:22\u001b[0m, in \u001b[0;36mTokenizer.str_to_id\u001b[1;34m(self, str_token)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstr_to_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, str_token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     21\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(str_token)\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoding) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgiven string \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be tokenized to exactly one token\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoding[\u001b[38;5;241m0\u001b[39m]\n",
                        "\u001b[1;31mAssertionError\u001b[0m: given string partly should be tokenized to exactly one token"
                    ]
                }
            ],
            "source": [
                "def print_trie(node, depth: int) -> None:\n",
                "    print(\" \" * depth + \"'\" + node.value.token + \"'\")\n",
                "    for str_token, child in node.children.items():\n",
                "        assert str_token == child.value.token, f\"'{str_token}' != '{node.value.token}'\"\n",
                "        print_trie(child, depth + 1)\n",
                "\n",
                "\n",
                "n2g_model = sm_n2g_models[0]\n",
                "root = n2g_model.trie_root\n",
                "\n",
                "gelu_tokenizer = Tokenizer(model_gelu)\n",
                "rs_model = FeatureModel.from_model(gelu_tokenizer, n2g_model)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "deepdecipher",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
