defaults:
  - mas
  - _self_

out_path: outputs/gpt2-small-mas_store
dataset_name: 'Skylion007/openwebtext'
model_name: 'gpt2-small'

layers:
  - hook_id: blocks.0.hook_mlp_out
    num_features: 768
  - hook_id: blocks.6.hook_mlp_out
    num_features: 768
  - hook_id: blocks.11.hook_mlp_out
    num_features: 768
  - hook_id: blocks.0.hook_mlp_out
    sae_hf_repo: jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs
    sae_file: v5_32k_layer_0/sae_weights.safetensors
  - hook_id: blocks.5.hook_mlp_out
    sae_hf_repo: jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs
    sae_file: v5_32k_layer_5/sae_weights.safetensors
  - hook_id: blocks.11.hook_mlp_out
    sae_hf_repo: jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs
    sae_file: v5_32k_layer_11/sae_weights.safetensors

params:
  high_activation_weighting: 1.0
  firing_threshold: 0.5
  sample_overlap: 256
  num_max_samples: 32
  sample_length_pre: 192
  sample_length_post: 64
  seed: 0
  activation_bins:
    ranges:
      - start: 0.0
        end: 6.0
        num_bins: 31
