\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

\Acp{SAE} have emerged as a promising approach for extracting interpretable features from \acp{LLM} in an unsupervised manner.
We provide a comprehensive review of \ac{SAE} literature in transformer model interpretability, and introduce the novel application of the \ac{N2G} method as a measure of interpretability.
Using \ac{N2G} in this way, we replicate several key findings.
We confirm that \ac{SAE} latents are more interpretable than \ac{MLP} neurons, we identify clusters of \ac{SAE} latents based on density and interpretability, revealing an unexpected ultra-high density cluster, and we corroborate the phenomenon of highly similar feature directions within a extreme density, low interpretability cluster.
This work contributes to ongoing efforts to understand \acp{LLM}, aiming to enhance their safety and alignment with human values.
