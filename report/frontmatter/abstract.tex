\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

Sparse Autoencoders (SAEs) have emerged as a promising approach for extracting interpretable features from Large Language Models (LLMs) in an unsupervised manner.
We provide a comprehensive review of SAE literature in transformer model interpretability, and introduce the novel application of the Neuron2Graph (N2G) method as a measure of interpretability.
Using N2G in this way, we replicate several key findings.
We confirm that SAE latents are more interpretable than MLP neurons, we identify clusters of SAE latents based on density and interpretability, revealing an unexpected ultra-high density cluster, and we corroborate the phenomenon of highly similar feature directions within a extreme density, low interpretability cluster.
This work contributes to ongoing efforts to understand LLMs, aiming to enhance their safety and alignment with human values.
