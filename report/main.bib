
@misc{foote_neuron_2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\EHVYIWBD\\Foote et al. - 2023 - Neuron to Graph Interpreting Language Model Neuro.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\XLRS23N4\\2305.html:text/html},
}

@misc{garde_deepdecipher_2023,
	title = {{DeepDecipher}: {Accessing} and {Investigating} {Neuron} {Activation} in {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Licence (CC-BY-NC-ND)},
	shorttitle = {{DeepDecipher}},
	url = {http://arxiv.org/abs/2310.01870},
	doi = {10.48550/arXiv.2310.01870},
	abstract = {As large language models (LLMs) become more capable, there is an urgent need for interpretable and transparent tools. Current methods are difficult to implement, and accessible tools to analyze model internals are lacking. To bridge this gap, we present DeepDecipher - an API and interface for probing neurons in transformer models' MLP layers. DeepDecipher makes the outputs of advanced interpretability techniques for LLMs readily available. The easy-to-use interface also makes inspecting these complex models more intuitive. This paper outlines DeepDecipher's design and capabilities. We demonstrate how to analyze neurons, compare models, and gain insights into model behavior. For example, we contrast DeepDecipher's functionality with similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher enables efficient, scalable analysis of LLMs. By granting access to state-of-the-art interpretability methods, DeepDecipher makes LLMs more transparent, trustworthy, and safe. Researchers, engineers, and developers can quickly diagnose issues, audit systems, and advance the field.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Garde, Albert and Kran, Esben and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2310.01870 [cs]},
	keywords = {Computer Science - Machine Learning, 68T50 (Primary) 68T05 (Secondary), I.2.7},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\3RIPKI9H\\Garde et al. - 2023 - DeepDecipher Accessing and Investigating Neuron A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\HPGFUTED\\2310.html:text/html},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\GGYGXFZ5\\Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\MPIW2KKM\\2309.html:text/html},
}

@misc{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	doi = {10.48550/arXiv.2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv:2002.12327 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\9H3J65NM\\Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\44E2CYUZ\\2002.html:text/html},
}

@article{elhage_toy_2022,
	title = {Toy {Models} of {Superposition}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
	year = {2022},
}

@article{bricken_towards_2023,
	title = {Towards {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}},
	journal = {Transformer Circuits Thread},
	author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
	year = {2023},
}

@misc{conmy_towards_2023,
	title = {Towards {Automated} {Circuit} {Discovery} for {Mechanistic} {Interpretability}},
	url = {http://arxiv.org/abs/2304.14997},
	doi = {10.48550/arXiv.2304.14997},
	abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.},
	urldate = {2024-02-12},
	publisher = {arXiv},
	author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
	month = oct,
	year = {2023},
	note = {arXiv:2304.14997 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\2FWBXADC\\Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\MNJ469VS\\2304.html:text/html},
}

@misc{bolukbasi_interpretability_2021,
	title = {An {Interpretability} {Illusion} for {BERT}},
	url = {http://arxiv.org/abs/2104.07143},
	doi = {10.48550/arXiv.2104.07143},
	abstract = {We describe an "interpretability illusion" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.},
	urldate = {2024-02-12},
	publisher = {arXiv},
	author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Viégas, Fernanda and Wattenberg, Martin},
	month = apr,
	year = {2021},
	note = {arXiv:2104.07143 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\X3L3IRUW\\Bolukbasi et al. - 2021 - An Interpretability Illusion for BERT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\GKKVL2M3\\2104.html:text/html},
}

@misc{hao_self-attention_2021,
	title = {Self-{Attention} {Attribution}: {Interpreting} {Information} {Interactions} {Inside} {Transformer}},
	shorttitle = {Self-{Attention} {Attribution}},
	url = {http://arxiv.org/abs/2004.11207},
	doi = {10.48550/arXiv.2004.11207},
	abstract = {The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
	month = feb,
	year = {2021},
	note = {arXiv:2004.11207 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\IBJ5ZJMU\\Hao et al. - 2021 - Self-Attention Attribution Interpreting Informati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\6UC5SYBS\\2004.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\R6C7FF8F\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\G6B5WF34\\1706.html:text/html},
}

@article{elhage_mathematical_2021,
	title = {A {Mathematical} {Framework} for {Transformer} {Circuits}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2021},
}

@article{olsson_-context_2022,
	title = {In-context {Learning} and {Induction} {Heads}},
	journal = {Transformer Circuits Thread},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2022},
}

@article{bills_language_2023,
	title = {Language models can explain neurons in language models},
	journal = {URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)},
	author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
	year = {2023},
}

@article{riggs_improving_2024,
	title = {Improving {SAE}'s by {Sqrt}()-ing {L1} \& {Removing} {Lowest} {Activating} {Features}},
	url = {https://www.alignmentforum.org/posts/YiGs8qJ8aNBgwt2YN/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest},
	abstract = {TL;DR
We achieve better SAE performance by: …},
	language = {en},
	urldate = {2024-05-29},
	author = {Riggs, Logan and Brinkmann, Jannik},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\DFF8YG2W\\improving-sae-s-by-sqrt-ing-l1-and-removing-lowest.html:text/html},
}

@article{robert_aizi_research_2024,
	title = {Research {Report}: {Sparse} {Autoencoders} find only 9/180 board state features in {OthelloGPT}},
	shorttitle = {Research {Report}},
	url = {https://www.lesswrong.com/posts/BduCMgmjJnCtc7jKc/research-report-sparse-autoencoders-find-only-9-180-board},
	abstract = {[3/7 Edit: I have rephrased the bolded claims in the abstract per this comment from Joseph Bloom, hopefully improving the heat-to-light ratio.  …},
	language = {en},
	urldate = {2024-05-29},
	author = {Robert\_AIZI},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\8F5UHS7M\\research-report-sparse-autoencoders-find-only-9-180-board.html:text/html},
}

@article{riggs_finding_2023,
	title = {Finding {Sparse} {Linear} {Connections} between {Features} in {LLMs}},
	url = {https://www.alignmentforum.org/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms},
	abstract = {TL;DR: We use SGD to find sparse connections between features; additionally a large fraction of features between the residual stream \& MLP can be mod…},
	language = {en},
	urldate = {2024-05-29},
	author = {Riggs, Logan and Mitchell, Sam and Eccentricity},
	month = dec,
	year = {2023},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\XYNRDPBS\\finding-sparse-linear-connections-between-features-in-llms.html:text/html},
}

@article{vaintrob_toward_2024,
	title = {Toward {A} {Mathematical} {Framework} for {Computation} in {Superposition}},
	url = {https://www.alignmentforum.org/posts/2roZtSr5TGmLjXMnT/toward-a-mathematical-framework-for-computation-in},
	abstract = {Author order randomized. Authors contributed roughly equally — see attribution section for details. …},
	language = {en},
	urldate = {2024-05-29},
	author = {Vaintrob, Dmitry and jake\_mendel and Kaarel},
	month = jan,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\THGKRULI\\toward-a-mathematical-framework-for-computation-in.html:text/html},
}

@article{wright_addressing_2024,
	title = {Addressing {Feature} {Suppression} in {SAEs}},
	url = {https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes},
	abstract = {Produced as part of the ML Alignment Theory Scholars Program - Winter 2023-24 Cohort as part of Lee Sharkey's stream. …},
	language = {en},
	urldate = {2024-05-29},
	author = {Wright, Benjamin and Sharkey, Lee},
	month = feb,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\8RG94LGD\\addressing-feature-suppression-in-saes.html:text/html},
}

@article{lin_announcing_2024,
	title = {Announcing {Neuronpedia}: {Platform} for accelerating research into {Sparse} {Autoencoders}},
	shorttitle = {Announcing {Neuronpedia}},
	url = {https://www.alignmentforum.org/posts/BaEQoxHhWPrkinmxd/announcing-neuronpedia-platform-for-accelerating-research},
	abstract = {This posts assumes basic familiarity with Sparse Autoencoders. For those unfamiliar with this technique, we highly recommend the introductory section…},
	language = {en},
	urldate = {2024-05-29},
	author = {Lin, Johnny and Bloom, Joseph},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\ZL5WVYU4\\announcing-neuronpedia-platform-for-accelerating-research.html:text/html},
}

@article{bloom_understanding_2024,
	title = {Understanding {SAE} {Features} with the {Logit} {Lens}},
	url = {https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-with-the-logit-lens},
	abstract = {This work was produced as part of the ML Alignment \& Theory Scholars Program - Winter 2023-24 Cohort, with support from Neel Nanda and Arthur Conmy.…},
	language = {en},
	urldate = {2024-05-29},
	author = {Bloom, Joseph and Lin, Johnny},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\NY2F6V8Y\\understanding-sae-features-with-the-logit-lens.html:text/html},
}

@article{robertzk_we_2024,
	title = {We {Inspected} {Every} {Head} {In} {GPT}-2 {Small} using {SAEs} {So} {You} {Don}’t {Have} {To}},
	url = {https://www.alignmentforum.org/posts/xmegeW5mqiBsvoaim/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don},
	abstract = {This is an interim report that we are currently building on. We hope this update will be useful to related research occurring in parallel. Produced a…},
	language = {en},
	urldate = {2024-05-29},
	author = {robertzk and Kissane, Connor and Conmy, Arthur and Nanda, Neel},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\LVHE8TG5\\we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don.html:text/html},
}

@misc{engels_not_2024,
	title = {Not {All} {Language} {Model} {Features} {Are} {Linear}},
	url = {https://arxiv.org/abs/2405.14860v1},
	abstract = {Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Finally, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we find further circular representations by breaking down the hidden states for these tasks into interpretable components.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Engels, Joshua and Liao, Isaac and Michaud, Eric J. and Gurnee, Wes and Tegmark, Max},
	month = may,
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\alber\\Zotero\\storage\\F4GD9DJD\\Engels et al. - 2024 - Not All Language Model Features Are Linear.pdf:application/pdf},
}

@misc{deng_measuring_2023,
	title = {Measuring {Feature} {Sparsity} in {Language} {Models}},
	url = {https://arxiv.org/abs/2310.07837v2},
	abstract = {Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Deng, Mingyang and Tao, Lucas and Benton, Joe},
	month = oct,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\alber\\Zotero\\storage\\IPYA3B8I\\Deng et al. - 2023 - Measuring Feature Sparsity in Language Models.pdf:application/pdf},
}

@inproceedings{wang_interpretability_2022,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 {Small}},
	shorttitle = {Interpretability in the {Wild}},
	url = {https://openreview.net/forum?id=NpsVSN6o4ul},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria - faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	language = {en},
	urldate = {2024-05-29},
	author = {Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = sep,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\alber\\Zotero\\storage\\8HWH7FI7\\Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indire.pdf:application/pdf},
}

@misc{jorgensen_improving_2023,
	title = {Improving {Activation} {Steering} in {Language} {Models} with {Mean}-{Centring}},
	url = {https://arxiv.org/abs/2312.03813v1},
	abstract = {Recent work in activation steering has demonstrated the potential to better control the outputs of Large Language Models (LLMs), but it involves finding steering vectors. This is difficult because engineers do not typically know how features are represented in these models. We seek to address this issue by applying the idea of mean-centring to steering vectors. We find that taking the average of activations associated with a target dataset, and then subtracting the mean of all training activations, results in effective steering vectors. We test this method on a variety of models on natural language tasks by steering away from generating toxic text, and steering the completion of a story towards a target genre. We also apply mean-centring to extract function vectors, more effectively triggering the execution of a range of natural language tasks by a significant margin (compared to previous baselines). This suggests that mean-centring can be used to easily improve the effectiveness of activation steering in a wide range of contexts.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Jorgensen, Ole and Cope, Dylan and Schoots, Nandi and Shanahan, Murray},
	month = dec,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\alber\\Zotero\\storage\\7PK92NAM\\Jorgensen et al. - 2023 - Improving Activation Steering in Language Models w.pdf:application/pdf},
}

@misc{bushnaq_local_2024,
	title = {The {Local} {Interaction} {Basis}: {Identifying} {Computationally}-{Relevant} and {Sparsely} {Interacting} {Features} in {Neural} {Networks}},
	shorttitle = {The {Local} {Interaction} {Basis}},
	url = {https://arxiv.org/abs/2405.10928v2},
	abstract = {Mechanistic interpretability aims to understand the behavior of neural networks by reverse-engineering their internal computations. However, current methods struggle to find clear interpretations of neural network activations because a decomposition of activations into computational features is missing. Individual neurons or model components do not cleanly correspond to distinct features or functions. We present a novel interpretability method that aims to overcome this limitation by transforming the activations of the network into a new basis - the Local Interaction Basis (LIB). LIB aims to identify computational features by removing irrelevant activations and interactions. Our method drops irrelevant activation directions and aligns the basis with the singular vectors of the Jacobian matrix between adjacent layers. It also scales features based on their importance for downstream computation, producing an interaction graph that shows all computationally-relevant features and interactions in a model. We evaluate the effectiveness of LIB on modular addition and CIFAR-10 models, finding that it identifies more computationally-relevant features that interact more sparsely, compared to principal component analysis. However, LIB does not yield substantial improvements in interpretability or interaction sparsity when applied to language models. We conclude that LIB is a promising theory-driven approach for analyzing neural networks, but in its current form is not applicable to large language models.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Bushnaq, Lucius and Heimersheim, Stefan and Goldowsky-Dill, Nicholas and Braun, Dan and Mendel, Jake and Hänni, Kaarel and Griffin, Avery and Stöhler, Jörn and Wache, Magdalena and Hobbhahn, Marius},
	month = may,
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\alber\\Zotero\\storage\\8YPM95AE\\Bushnaq et al. - 2024 - The Local Interaction Basis Identifying Computati.pdf:application/pdf},
}

@misc{park_linear_2023,
	title = {The {Linear} {Representation} {Hypothesis} and the {Geometry} of {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2311.03658v1},
	abstract = {Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
	month = nov,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\alber\\Zotero\\storage\\4JTNAQH3\\Park et al. - 2023 - The Linear Representation Hypothesis and the Geome.pdf:application/pdf},
}

@misc{gurnee_universal_2024,
	title = {Universal {Neurons} in {GPT2} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.12181},
	abstract = {A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms. In other words, are neural mechanisms universal across different models? In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable. In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs. We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families. We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12181 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {2401.pdf:C\:\\Users\\alber\\Zotero\\storage\\VJ6EWDT6\\2401.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\CK3JFS6E\\2401.html:text/html},
}

@article{sharkey_interim_2022,
	title = {[{Interim} research report] {Taking} features out of superposition with sparse autoencoders},
	url = {https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition},
	abstract = {We're thankful for helpful comments from Trenton Bricken, Eric Winsor, Noa Nabeshima, and Sid Black.  …},
	language = {en},
	urldate = {2024-05-30},
	author = {Sharkey, Lee and Braun, Dan and beren},
	month = dec,
	year = {2022},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\X4I9VTV3\\interim-research-report-taking-features-out-of-superposition.html:text/html},
}

@misc{wright_high-dimensional_2022,
	title = {High-{Dimensional} {Data} {Analysis} with {Low}-{Dimensional} {Models}: {Principles}, {Computation}, and {Applications}},
	shorttitle = {High-{Dimensional} {Data} {Analysis} with {Low}-{Dimensional} {Models}},
	url = {https://www.cambridge.org/highereducation/books/high-dimensional-data-analysis-with-low-dimensional-models/05E9DE014DCE9E08D4AA014975D94B60},
	abstract = {Connecting theory with practice, this systematic and rigorous introduction covers the fundamental principles, algorithms and applications of key mathematical models for high-dimensional data analysis. Comprehensive in its approach, it provides unified coverage of many different low-dimensional models and analytical techniques, including sparse and low-rank models, and both convex and non-convex formulations. Readers will learn how to develop efficient and scalable algorithms for solving real-world problems, supported by numerous examples and exercises throughout, and how to use the computational tools learnt in several application contexts. Applications presented include scientific imaging, communication, face recognition, 3D vision, and deep networks for classification. With code available online, this is an ideal textbook for senior and graduate students in computer science, data science, and electrical engineering, as well as for those taking courses on sparsity, low-dimensional structures, and high-dimensional data. Foreword by Emmanuel Candès.},
	language = {en},
	urldate = {2024-05-30},
	journal = {Higher Education from Cambridge University Press},
	author = {Wright, John and Ma, Yi},
	month = jan,
	year = {2022},
	doi = {10.1017/9781108779302},
	note = {ISBN: 9781108779302
Publisher: Cambridge University Press},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\65W5IRBW\\05E9DE014DCE9E08D4AA014975D94B60.html:text/html},
}

@article{olshausen_sparse_1997,
	title = {Sparse coding with an overcomplete basis set: a strategy employed by {V1}?},
	volume = {37},
	issn = {0042-6989},
	shorttitle = {Sparse coding with an overcomplete basis set},
	doi = {10.1016/s0042-6989(97)00169-7},
	abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
	language = {eng},
	number = {23},
	journal = {Vision Research},
	author = {Olshausen, B. A. and Field, D. J.},
	month = dec,
	year = {1997},
	pmid = {9425546},
	keywords = {Algorithms, Animals, Mammals, Models, Psychological, Visual Cortex, Visual Perception},
	pages = {3311--3325},
}

@article{conmy_my_2023,
	title = {My best guess at the important tricks for training {1L} {SAEs}},
	url = {https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes},
	abstract = {TL;DR: this quickly-written post gives a list of my guesses of the most important parts of training a Sparse Autoencoder on a 1L Transformer, with op…},
	language = {en},
	urldate = {2024-05-30},
	author = {Conmy, Arthur},
	month = dec,
	year = {2023},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\S7K3VH4S\\my-best-guess-at-the-important-tricks-for-training-1l-saes.html:text/html},
}

@misc{rajamanoharan_improving_2024,
	title = {Improving {Dictionary} {Learning} with {Gated} {Sparse} {Autoencoders}},
	url = {http://arxiv.org/abs/2404.16014},
	doi = {10.48550/arXiv.2404.16014},
	abstract = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kramár, János and Shah, Rohin and Nanda, Neel},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16014 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\6IF5FWLD\\Rajamanoharan et al. - 2024 - Improving Dictionary Learning with Gated Sparse Au.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\CGSN5Q7N\\2404.html:text/html},
}

@article{taggart_prolu_2024,
	title = {{ProLU}: {A} {Nonlinearity} for {Sparse} {Autoencoders}},
	shorttitle = {{ProLU}},
	url = {https://www.alignmentforum.org/posts/HEpufTdakGTTKgoYF/prolu-a-nonlinearity-for-sparse-autoencoders},
	abstract = {Abstract
This paper presents ProLU, an alternative to ReLU for the activation function in sparse autoencoders that produces a pareto improvement over…},
	language = {en},
	urldate = {2024-05-30},
	author = {Taggart, Glen},
	month = apr,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\NJGMVKPI\\prolu-a-nonlinearity-for-sparse-autoencoders.html:text/html},
}

@misc{bereska_mechanistic_2024,
	title = {Mechanistic {Interpretability} for {AI} {Safety} -- {A} {Review}},
	url = {http://arxiv.org/abs/2404.14082},
	doi = {10.48550/arXiv.2404.14082},
	abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Bereska, Leonard and Gavves, Efstratios},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14082 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\LLM3K49F\\Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety -- A Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\7QJ9TRIE\\2404.html:text/html},
}

@misc{black_interpreting_2022,
	title = {Interpreting {Neural} {Networks} through the {Polytope} {Lens}},
	url = {http://arxiv.org/abs/2211.12312},
	doi = {10.48550/arXiv.2211.12312},
	abstract = {Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description: directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a specific feature to a neural unit. In order to find a basic unit of description that does not suffer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the polytope lens. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classifiers and language models. Specifically, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reflect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Black, Sid and Sharkey, Lee and Grinsztajn, Leo and Winsor, Eric and Braun, Dan and Merizian, Jacob and Parker, Kip and Guevara, Carlos Ramón and Millidge, Beren and Alfour, Gabriel and Leahy, Connor},
	month = nov,
	year = {2022},
	note = {arXiv:2211.12312 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\I6RWZ3BY\\Black et al. - 2022 - Interpreting Neural Networks through the Polytope .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\LDJAQRUU\\2211.html:text/html},
}

@misc{makelov_towards_2024,
	title = {Towards {Principled} {Evaluations} of {Sparse} {Autoencoders} for {Interpretability} and {Control}},
	url = {http://arxiv.org/abs/2405.08366},
	doi = {10.48550/arXiv.2405.08366},
	abstract = {Disentangling model activations into meaningful features is a central problem in interpretability. However, the absence of ground-truth for these features in realistic scenarios makes validating recent approaches, such as sparse dictionary learning, elusive. To address this challenge, we propose a framework for evaluating feature dictionaries in the context of specific tasks, by comparing them against {\textbackslash}emph\{supervised\} feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes. We apply this framework to the indirect object identification (IOI) task using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets. We find that these SAEs capture interpretable features for the IOI task, but they are less successful than supervised features in controlling the model. Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller, less interpretable features). We hope that our framework will provide a useful step towards more objective and grounded evaluations of sparse dictionary learning methods.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Makelov, Aleksandar and Lange, George and Nanda, Neel},
	month = may,
	year = {2024},
	note = {arXiv:2405.08366 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\E8T4DFET\\Makelov et al. - 2024 - Towards Principled Evaluations of Sparse Autoencod.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\963U7QQV\\2405.html:text/html},
}

@misc{marks_sparse_2024,
	title = {Sparse {Feature} {Circuits}: {Discovering} and {Editing} {Interpretable} {Causal} {Graphs} in {Language} {Models}},
	shorttitle = {Sparse {Feature} {Circuits}},
	url = {http://arxiv.org/abs/2403.19647},
	doi = {10.48550/arXiv.2403.19647},
	abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19647 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\C4NGQC5Y\\Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\6TXVPXGR\\2403.html:text/html},
}

@article{marks_discriminating_2024,
	title = {Discriminating {Behaviorally} {Identical} {Classifiers}: a model problem for applying interpretability to scalable oversight},
	shorttitle = {Discriminating {Behaviorally} {Identical} {Classifiers}},
	url = {https://www.alignmentforum.org/posts/s7uD3tzHMvD868ehr/discriminating-behaviorally-identical-classifiers-a-model},
	abstract = {In a new preprint, Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models, my coauthors and I introduce a te…},
	language = {en},
	urldate = {2024-05-30},
	author = {Marks, Sam},
	month = apr,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\KTC49Q3A\\discriminating-behaviorally-identical-classifiers-a-model.html:text/html},
}

@article{templeton_scaling_2024,
	title = {Scaling {Monosemanticity}: {Extracting} {Interpretable} {Features} from {Claude} 3 {Sonnet}},
	url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
	journal = {Transformer Circuits Thread},
	author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
	year = {2024},
}

@misc{anthropic_introducing_nodate,
	title = {Introducing the next generation of {Claude}},
	url = {https://www.anthropic.com/news/claude-3-family},
	abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
	language = {en},
	urldate = {2024-05-30},
	author = {Anthropic},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\BVVN2R2Q\\claude-3-family.html:text/html},
}

@article{stephen_casper_eis_2024,
	title = {{EIS} {XIII}: {Reflections} on {Anthropic}’s {SAE} {Research} {Circa} {May} 2024},
	shorttitle = {{EIS} {XIII}},
	url = {https://www.alignmentforum.org/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may},
	abstract = {Part 13 of 12 in the Engineer’s Interpretability Sequence. …},
	language = {en},
	urldate = {2024-05-31},
	author = {Stephen Casper},
	month = may,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\T4IIZNAB\\eis-xiii-reflections-on-anthropic-s-sae-research-circa-may.html:text/html},
}

@article{hugofry_towards_2024,
	title = {Towards {Multimodal} {Interpretability}: {Learning} {Sparse} {Interpretable} {Features} in {Vision} {Transformers}},
	shorttitle = {Towards {Multimodal} {Interpretability}},
	url = {https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2},
	abstract = {Executive Summary
In this post I present my results from training a Sparse Autoencoder (SAE) on a CLIP Vision Transformer (ViT) using the ImageNet-1k…},
	language = {en},
	urldate = {2024-05-31},
	author = {hugofry},
	month = apr,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\RALC3C54\\towards-multimodal-interpretability-learning-sparse-2.html:text/html},
}

@misc{dao_adversarial_2023,
	title = {An {Adversarial} {Example} for {Direct} {Logit} {Attribution}: {Memory} {Management} in gelu-4l},
	shorttitle = {An {Adversarial} {Example} for {Direct} {Logit} {Attribution}},
	url = {http://arxiv.org/abs/2310.07325},
	doi = {10.48550/arXiv.2310.07325},
	abstract = {How do language models deal with the limited bandwidth of the residual stream? Prior work has suggested that some attention heads and MLP layers may perform a "memory management" role. That is, clearing residual stream directions set by earlier layers by reading in information and writing out the negative version. In this work, we present concrete evidence for this phenomenon in a 4-layer transformer. We identify several heads in layer 2 that consistently remove the output of a single layer 0 head. We then verify that this erasure causally depends on the original written direction. We further demonstrate that direct logit attribution (DLA) suggests that writing and erasing heads directly contribute to predictions, when in fact their effects cancel out. Then we present adversarial prompts for which this effect is particularly salient. These findings reveal that memory management can make DLA results misleading. Accordingly, we make concrete recommendations for circuit analysis to prevent interpretability illusions.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Dao, James and Lau, Yeu-Tong and Rager, Can and Janiak, Jett},
	month = nov,
	year = {2023},
	note = {arXiv:2310.07325 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\VVJ78KJJ\\Dao et al. - 2023 - An Adversarial Example for Direct Logit Attributio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\7TVSX728\\2310.html:text/html},
}

@misc{belrose_eliciting_2023,
	title = {Eliciting {Latent} {Predictions} from {Transformers} with the {Tuned} {Lens}},
	url = {http://arxiv.org/abs/2303.08112},
	doi = {10.48550/arXiv.2303.08112},
	abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the {\textbackslash}emph\{tuned lens\}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
	month = nov,
	year = {2023},
	note = {arXiv:2303.08112 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\IUGATVYR\\Belrose et al. - 2023 - Eliciting Latent Predictions from Transformers wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\6XCJJ83B\\2303.html:text/html},
}

@article{nostalgebraist_interpreting_2020,
	title = {interpreting {GPT}: the logit lens},
	shorttitle = {interpreting {GPT}},
	url = {https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
	abstract = {This post relates an observation I've made in my work with GPT-2, which I have not seen made elsewhere. …},
	language = {en},
	urldate = {2024-05-31},
	author = {nostalgebraist},
	month = aug,
	year = {2020},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\MIXF9SZP\\interpreting-gpt-the-logit-lens.html:text/html},
}

@article{kissane_attention_2024,
	title = {Attention {SAEs} {Scale} to {GPT}-2 {Small}},
	url = {https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr/attention-saes-scale-to-gpt-2-small},
	abstract = {This is an interim report that we are currently building on. We hope this update + open sourcing our SAEs will be useful to related research occurrin…},
	language = {en},
	urldate = {2024-05-31},
	author = {Kissane, Connor and robertzk and Conmy, Arthur and Nanda, Neel},
	month = feb,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\JJPS7KH9\\attention-saes-scale-to-gpt-2-small.html:text/html},
}

@article{kissane_sparse_2024,
	title = {Sparse {Autoencoders} {Work} on {Attention} {Layer} {Outputs}},
	url = {https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs},
	abstract = {This post is the result of a 2 week research sprint project during the training phase of Neel Nanda’s MATS stream.  …},
	language = {en},
	urldate = {2024-05-31},
	author = {Kissane, Connor and robertzk and Conmy, Arthur and Nanda, Neel},
	month = jan,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\9M4EDPGZ\\sparse-autoencoders-work-on-attention-layer-outputs.html:text/html},
}
