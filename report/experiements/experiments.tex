\documentclass[../main.tex]{subfiles}


\begin{document}


\subsection{Interpretability score comparison}
\subsubsection{Question}
Is the set of neurons/features that N2G has difficulty interpreting the same as the set of neurons/features that are difficult for other methods?

\subsubsection{Method}
\begin{itemize}
    \item Acquire some SAEs.
    \item Train and evaluate N2G on a subset of the features of the SAEs.
    \item Acquire interpretability scores for other methods on the same features.
    \begin{itemize}
        \item Examples include: Automated interpretability, manual evaluation, and feature density.
        \item We can only compute the last one ourselves.
    \end{itemize}
    \item Compare the interpretability scores of N2G with the interpretability scores of other methods.
    \begin{itemize}
        \item I should design the statistical test beforehand.
        \item I expect the means and variances to be different.
        What I'm interested in is whether they disagree on which are interpretable and which are not.
    \end{itemize}
\end{itemize}
We use the word "acquire" since we will likely run some methods ourselves while getting data for others from the literature.

\subsubsection{Considerations}
\begin{itemize}
    \item Positive
    \begin{itemize}
        \item This would be by far the easiest experiment to do.
    \end{itemize}
    \item Negative
    \begin{itemize}
        \item It may not be possible to get the data for the other methods. Other papers have used them on the same features, but they have not published the data.
    \end{itemize}
\end{itemize}

\subsection{N2G task performance}
\subsubsection{Question}
Given a task the model can solve, can N2G guide us in changing performance? Is there difference when working with SAE features vs. MLP neurons directly?

\subsubsection{Method}
\begin{itemize}
    \item Acquire some SAEs.
    \item Train N2G on both the features of the SAE and the MLP neurons.
    \item Find a simple syntactical task and a way of guessing what features are responsible using their N2Gs.
    \begin{itemize}
        \item A possible example is to continue "The cat is sm" with a token that creates a word.
    \end{itemize}
    \item Perform task with and without those features ablated.
    \begin{itemize}
        \item Do try with multiple kinds of ablation.
    \end{itemize}
    \item Compare the performance with and without features and between MLP neurons and SAE neurons.
    \begin{itemize}
        \item As always, a statistical test should be designed beforehand.
    \end{itemize}
\end{itemize}

\subsubsection{Considerations}
\begin{itemize}
    \item Positive
    \begin{itemize}
        \item Should be pretty simple once I have a task and the N2Gs.
        \item Would be an interesting support/conterpoint to the rest of the literature showing that various methods work better on the features of SAEs
    \end{itemize}
    \item Negative
    \begin{itemize}
        \item It may be difficult to find any task that N2G can solve.
        A test on the N2Gs of the gelu-1l MLP neurons showed no effect.
        \item The gpt2-small SAEs I found may be too large to get N2Gs for all features, and it will likely be far harder to steer a model with only some features.
    \end{itemize}
\end{itemize}


\subsection{Feature universality}
\subsubsection{Question}
Do the same features appear in different models?
Does this hold to a lesser or higher degree with SAE features vs. MLP neurons?

Interesting sub question: Is there a connection between characteristics of the N2G (e.g. performance, size...) and the universality of the feature? I.e., does N2G perform better on more universal features?

\subsubsection{Method}
\begin{itemize}
    \item Acquire some SAEs for several different models.
    \item Train N2G on both MLP neurons and the features of the SAEs.
    \item Compare N2G models across models and look for features that often appear.
\end{itemize}
Could also be tested across layers of the same model.

\subsubsection{Considerations}
\begin{itemize}
    \item Positive
    \begin{itemize}
        \item It feels like finding something interesting to say is pretty likely?
    \end{itemize}
    \item Negative
    \begin{itemize}
        \item We would have to find an appropriate definition of N2G similarity, which may be challenging.
        \item We only have two models to compare.
    \end{itemize}
\end{itemize}

\subbib{../../main}
\end{document}