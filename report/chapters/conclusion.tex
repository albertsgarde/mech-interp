In this thesis, we have explored several aspects of \acp{SAE} as used in the context of interpretability of language models.
We investigate the use of a novel measure of interpretability, \ac{N2G}, and use it to corroborate several findings from previous work.

First and foremost, find that \ac{N2G} performs better on \ac{SAE} latents than on \ac{MLP} neurons, corroborating the finding that \ac{SAE} latents are more interpretable than \ac{MLP} neurons.

This replication lends credence to the robustness of the original findings and suggests that \ac{N2G} can serve as a viable alternative for assessing the interpretability of \ac{SAE} latents.

Secondly, our research identified clusters of \ac{SAE} latents based on their density and interpretability, aligning with previous studies.
However, a notable divergence from existing literature emerged in our findings: we observed an ultra-high density cluster instead of the previously reported ultra-low density cluster. 

Thirdly, we corroborated the unexplained phenomenon that the directions of the extreme density cluster exhibit remarkable similarity.
Further work into the causes of this could potentially shed light on the issue of "dead neurons" in \acp{SAE}.

Lastly, we observed that these directions were more distributed than those in the high-density cluster, they still demonstrated a degree of similarity that exceeded random chance.
Unfortunately we did not have time to dive deeper into this, but a more thorough investigation of the direction distribution could provide more conclusive results on whether it is true that nearly orthogonal directions are exploited by language models.

In conclusion, our work has provided further evidence that \ac{SAE} latents are more interpretable than \ac{MLP} neurons, has introduced a new method for measuring interpretability, and corroborated several other findings.
Additionally, several new questions have been raised that could be the subject of future research.
