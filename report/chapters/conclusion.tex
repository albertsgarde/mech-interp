
Our study of \acp{SAE} and the \ac{N2G} method has yielded insights into the structure of \ac{SAE} latents and the use of \ac{N2G} as a measure of interpretability.
We corroborated previous findings that \ac{SAE} latents are more interpretable than MLP neurons, specifically observing higher \ac{N2G} performance on \ac{SAE} latents compared to MLP neurons.
This aligns with the results of Bricken et al. (2023) and Cunningham et al. (2023), despite our use of a different interpretability measure, thus further strengthening the evidence for this foundational property of \acp{SAE}.

We also observed a clustering of \ac{SAE} latents based on density and interpretability, partially corroborating the findings of Bricken et al. (2023) and Nanda et al. (2023).
Amongst these are the correlation between density and interpretability, and the presence of a cluster of uninterpretable \ac{SAE} latents that represent nearly the same linear combination of \ac{MLP} neurons.
However, we also found some discrepancies in the specific characteristics of these clusters, highlighting the need for further investigation in this area.

Our investigation into \ac{N2G} revealed both potential benefits and limitations of this method.
\ac{N2G} offers a computationally efficient approach to assessing interpretability, which is particularly valuable for large-scale models where more resource-intensive methods may be impractical.
However, our results suggest that \ac{N2G} may be biased towards simple syntactical features.
Despite this, it is still a useful proxy for interpretability, as long as this caveat is taken into account.

Overall this work has made clear a number of avenues of further research, especially into the geometry of the uninterpretable \ac{SAE} latents and the behaviour of the \ac{N2G} method as a measure of interpretability.
