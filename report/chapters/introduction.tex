Large Language Models (LLMs) based on the transformer architecture \parencite{vaswani_attention_2023} have shown exceptional performance across a range of tasks, yet how they achieve such impressive results remains poorly understood.
As these models are increasingly deployed in real-world applications, concerns have been raised about the potential for harm \parencite{noauthor_statement_nodate}\parencite{hendrycks_overview_2023}.
These include the potential for bias, the potential for misuse by malicious actors, and existential risks from misaligned models \parencite{ngo_alignment_2024}.
This makes the opacity of these models a significant issue, since only having a black-box understanding of their behaviour limits our ability to foresee or guarantee against harmful behaviour in out of distribution scenarios.
Additionally, a greater understanding of these models may allow us to better correct these behaviours once they are identified.
The fields of interpretability and explainability aim to address this issue by providing insights into the internal workings of these models.
Many approaches have been developed within these fields.
\textcite{bereska_mechanistic_2024} provides a recent overview and a useful taxonomy, while \textcite{rauker_toward_2023} provides a more thorough survey including a list of previous reviews in the same area.
%While most work is focused on empirical work, \textcite{elhage_mathematical_2021} gives a theoretical perspective on the topic.

Transformer models contain both attention and multi-layer perceptron (MLP) layers and the latter is the focus of this thesis.
Previous attempts at interpreting MLP neurons have focused on understanding the behaviour of individual neurons\parencite{wang_interpretability_2022}, but as demonstrated in \textcite{elhage_toy_2022}, the behaviour of individual neurons often doesn't map onto human understandable concepts.
\textcite{bricken_towards_2023} shows a possible way forward by suggesting the features of SAEs (Sparse Autoencoders) trained on the MLP layer activations as alternative units of interpretability.
In this work, we focus on these along with the N2G (Neuron2Graph) \parencite{foote_neuron_2023} method.
This is an interesting combination, since if SAEs truly do provide more interpretable features and N2G truly does provide a useful representation of feature behaviour, we would expect this to be reflected when comparing N2G graphs for individual neurons against those for SAE features.
Indeed, \textcite{gao_scaling_2024} uses N2G to test the interpretability of features of their SAEs.

In this work we first provide a review of the literature on SAEs as applied to interpretability of transformer models as well as giving a short description of the N2G method.
The review differs from the reviews mentioned above by focussing exclusively on SAEs and including the most recent work.
To our knowledge there is no other review that does this.
This allows us to go into considerable detail, and since the field is so young, it means we can cover essentially all relevant work.
We then perform/try to answer the question \todo{insert experiment here}.

