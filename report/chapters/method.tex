In this chapter we describe the methods used in this work.
We first describe what SAEs we use.
Then we describe which text dataset we use, and how we obtain the samples needed for N2G from that dataset.
Afterwards we go into detail about the implementation of the N2G method, including the code changes needed from the original implementation as well as how we evaluate the resulting N2G models.
Finally we describe the statistical methods used to analyse the data gained from the features and from evaluating the N2G models.
Most code used in this work is available at \footnote{\url{https://github.com/albertsgarde/thesis}} while our modifications to the N2G code can be found at \footnote{\url{https://github.com/albertsgarde/Neuron2Graph}}.

\section{SAEs}
Since SAEs are finnicky to train, we will be using a pre-trained SAE, namely \textcite{nanda_neelnandasparse_autoencoder_nodate}.
Though the past months have seen the development of several open source SAE libraries \parencite{bloom_jbloomaussaelens_2024}\parencite{cooney_ai-safety-foundationsparse_autoencoder_2024}, we started this work before then and do not use any of them.
Attempts were made extend the experiments to SAEs on \texttt{gpt2-small}, specifically those provided at \textcite{wu_openaisparse_autoencoder_2024}, but they proved unsuccesful due to memory constraints making the process too time consuming.

\section{Dataset}
\label{sec:dataset}
Our experiments require data in the form of a number of samples for each feature.
The text dataset we use is the same as was originally used to train both the model and the SAE, namely \textcite{nanda_neelnandac4-code-20k_nodate}.
The dataset contains samples of varying lengths, but we need fixed lengths samples for both more convenient and more efficient processing.
We solve this by creating overlapping samples of fixed length within each dataset sample, padding with the padding token when necessary.
The overlap is to ensure that all tokens are included with some preceding context.

\subsection{Sampling}
\label{sec:sampling}
For N2G to run on a feature, it needs a set of samples that activate the feature.
Since the running time scales linearly in the number of samples, we would like to keep this number as low as possible.
At the same time, what samples N2G is run on has a large impact on the results.
In the original N2G paper \parencite{foote_neuron_2023}, maximum activating samples were used.
However, we found that this leads to many of the samples being very similar which pollutes the test set.
Because of this and arguments that maximum activating samples can be misleading \parencite{bolukbasi_interpretability_2021}, we instead use a weighted random sampling approach.
Like with maximum activating samples we assume that the samples that activate the feature most highly are the most important for understanding the feature, but we do not assume that they are the \emph{only} important samples.
Inspired by softmax, we assign each sample a weight equal to
\begin{align*}
    w=\e^{\alpha a}
\end{align*}
where $a$ is the maximum activation of the feature on that sample and $\alpha$ is a hyperparameter controlling how much to prioritize the most activating samples.
A \emph{key} is then calculated for each sample given by
\begin{align*}
    k=\xi^{\frac1w}-[a<c]
\end{align*}
where $\xi\sim\mathrm{Uniform}(0,1)$ is a random number and $[a<c]$ is $1$ when the activation is below a firing threshold and $0$ otherwise.
Since the first term is always between $0$ and $1$, the second term ensures that samples with activations above the threshold are always prioritized.
Sampling is then done by streaming through a part of the dataset, calculating the key for each sample, and keeping the $n$ samples with the highest keys.
The second term ($[a<c]$) was included after noticing that some samples with all zero activations were included, which is a waste since they do not contribute to either training or evaluation.
The implementation used for these experiments is available \footnote{\url{https://github.com/albertsgarde/thesis/blob/main/thesis/mas/weighted_samples_store.py}} in the repo.
For the results below, we used $\alpha=1$, $c=0.5$, and $n=32$.

\section{N2G}
Our code for training and evaluating N2G models is heavily based on the original implementation \parencite{foote_apartresearchneuron2graph_2024}.
However the original was a single script with no documentation and since we needed to work in depth with the code we have done considerable refactoring work to improve code quality.
In our fork, the code is split into many files, all functions are typed, all constants are configurable from outside the code, and we follow the standards enforced by Ruff \parencite{marsh_astral-shruff_2024}.
During this work we found many redundancies, the removal of which has likely sped up the code though we have not done any formal benchmarking.
We also found that the resulting N2Gs took up much more space than seemed reasonable, both in memory and on disk.
While the disk usage could be solved by compressing the files (N2G graphs compress very well), this would not solve the memory usage.
Our solution was to create a Rust implementation of the N2Gs.
While the memory impact is hard to measure, the compressed disk size is not and it fell by a factor of $10$.
This implementation does not support training, but can be used to evaluate N2Gs and produce the visual graph representations.
The algorithm thus works by first training the N2Gs using the Python implementation, before converting them to the Rust implmentation for storage.

\subsection{Evaluation}
Given a feature and a N2G model trained on that feature's training set, we evaluate the model on the test set.
This is done by getting it to predict for each token in each sample of the test set whether the feature will fire on that token, where firing is defined as the activation being above a threshold.
These predictions can then be compared to the actual activations of the feature to calculate recall, precision, and F1-score.

\section{Analysis}
After N2G models have been trained and evaluated for all MLP and SAE features, we can compare the two populations.
Our analysis is based on 4 main statistics for each feature: recall, precision, F1-score, and feature density.
The first three refer to the performance of the N2G model of the feature when predicting the feature's activations on the test set while the last one is the fraction of tokens that activate the feature.
To compare these statistics between the two populations, we implement two-sample bootstrap tests.
For SAE latents, along with these statistical representations of the behaviour of features on a dataset, we also look at the directions in MLP activation space these features represent.
These directions can be read directly from the encoding matrix of the SAE ($\mat W_e$ in \autoref{eq:sae_structure}).
We define a number of ad-hoc clusters based and compare the directions in these clusters using intra-cluster cosine similarity and distance to mean direction.

\subsection{Density}
\label{sec:density}
To calculate density, we first define a threshold for what counts as an "activating" feature.
In this work, we choose a threshold of $0.6$.
This is somewhat arbitrary, but seems like a good balance between including high activations and excluding noise.
Experiments with other thresholds such as $0.2$ showed that the results were not particularly sensitive to the choice of threshold.
After defining a threshold, we run the language model on the dataset and count the number of tokens where each feature has an activation above the threshold.
The density of a feature is then the number of such tokens divided by the total number of tokens in the dataset.
