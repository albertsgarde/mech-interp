% Overview

\section{SAEs}
Since SAEs are finnicky to train, we will be using a pre-trained SAE, namely \todo{cite}.
Though the past months have seen the development of several open source SAE libraries \todo{cite}, we started this work before then and do not use any of them.

\section{Dataset}
Our experiments require data in the form of a number of samples for each feature.
The text dataset we use is the same as was originally used to train both the model and the SAE, namely \todo{cite}.
The dataset contains samples of varying lengths, but we need fixed lengths samples for both more convenient and more efficient processing.
We solve this by creating overlapping samples of fixed length within each dataset sample, padding with the padding token when necessary.
The overlap is to ensure that all tokens are included with some preceding context.

\subsection{Sampling}
For N2G to run on a feature, it needs a set of samples that activate the feature.
Since the running time scales linearly in the number of samples, we would like to keep this number as low as possible.
At the same time, what samples N2G is run on has a large impact on the results.
In the original N2G paper \todo{cite}, maximum activating samples were used.
However, we found that this leads to many of the samples being very similar which pollutes the test set.
Because of this and arguments that maximum activating samples can be misleading \todo{cite}, we instead use a weighted random sampling approach.
Like with maximum activating samples we assume that the samples that activate the feature most highly are the most important for understanding the feature, but we do not assume that they are the \emph{only} important samples.
Inspired by softmax, we assign each sample a weight equal to
\begin{align*}
    w=\e^{\alpha a}
\end{align*}
where $a$ is the maximum activation of the feature on that sample and $\alpha$ is a hyperparameter controlling how much to prioritize the most activating samples.
A \emph{key} is then calculated for each sample given by
\begin{align*}
    k=\xi^{\frac1w}-[a<c]
\end{align*}
where $\xi\sim\mathrm{Uniform}(0,1)$ is a random number and $[a<c]$ is $1$ when the activation is below a firing threshold and $0$ otherwise.
Since the first term is always between $0$ and $1$, the second term ensures that samples with activations above the threshold are always prioritized.
Sampling is then done by streaming through a part of the dataset, calculating the key for each sample, and keeping the $n$ samples with the highest keys.
For the results below, we used $\alpha=1$, $c=0.5$, and $n=32$.

\section{N2G}
Our code for training and evaluating N2G models is heavily based on the original implementation \todo{cite}.
However the original was a single script with no documentation and since we needed to work in depth with the code we have done considerable refactoring work to improve code quality.
In our fork \todo{link} the code is split into many files, all functions are typed, all constants are configurable from outside the code, and we follow the standards enforced by Ruff \todo{cite}.
During this work we found many redundancies, the removal of which has likely sped up the code though we have not done any formal benchmarking.
We also found that the resulting N2Gs took up much more space than seemed reasonable, both in memory and on disk.
Our solution was to create a Rust implementation of the N2Gs.
This implementation does not support training, but can be used to evaluate N2Gs and produce the visual graph representations.
The algorithm thus works by first training the N2Gs using the Python implementation, before converting them to the Rust implmentation for storage.

\subsection{Evaluation}
Given a feature and a N2G model trained on that feature's training set, we evaluate the model on the test set.
This is done by getting it to predict for each token in each sample of the test set whether the feature will fire on that token, where firing is defined as the activation being above a threshold.
These predictions can then be compared to the actual activations of the feature to calculate recall, precision, and F1-score.

\section{Analysis}
After N2G models have been trained and evaluated for all MLP and SAE features, we can compare the two populations.
Since our goal is to compare the performance of N2G on the two populations, we are interested in the recall and precision of the models for each feature.




\begin{itemize}
    \item Acquire an SAE for a layer of the language model.
    \item Calculate MAS for the neurons of the layer and features of the SAE.
    \item Train N2G models for each neuron of the layer and feature of the SAE based on the calculated MAS.
    \item Evaluate all N2G models.
    \item Compare performance statistics between the two populations.
    \begin{itemize}
        \item Use bootstrap to compare recall, precision, and F1-score.
    \end{itemize}
\end{itemize}

