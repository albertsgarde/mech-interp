In this chapter we define the terminology used the rest of the report (section \ref{sec:preliminaries}) and present the prior work that serves as a basis for the thesis (section \ref{sec:n2g}).
We also provide a more in depth overview of the literature on SAEs (section \ref{sec:sae}).

\section{Preliminaries}
\label{sec:preliminaries}
Before we can dive into the specifics of the methods used in this thesis, 
we need to establish some basic concepts and terminology.
Firstly, throughout this section, we assume there is some fixed 
transformer language model \parencite{vaswani_attention_2023} 
which we want to interpret.
We will refer to this as the \emph{language model}.
We will also assume that we have a set of text samples 
which we use to train and test the methods.
We will refer to these as the \emph{dataset}.

Next we introduce the unit of interpretability, namely \emph{features}. 
In the most general form, a feature is any function of the activations 
of a model, but in the context of this thesis we will focus on two 
specific types.
The first type is the individual neurons of the model, i.e. the activations which can be read directly from the model, that are actually represented on the GPU during inference.
The second type is the latent neurons of an SAE (see section \ref{sec:sae}).
We will generally refer to features of the first type as \emph{neurons} and features of the second type as \emph{features of an SAE} or just \emph{features}.
\todo{Define sparsity and density}

\section{Neuron2Graph}
\label{sec:n2g}
The main interpretability method we use to investigate SAEs is N2G (Neuron2Graph)
as described in \cite{foote_neuron_2023}.
It attempts to build a graph model of the behaviours of a feature 
by finding a set of patterns which activate the feature.
Each pattern consists of a string of tokens (an $n$-gram) 
with the possibility of a special ignore token 
which signal that the token at that position does not matter.
In the original paper, this is done on the basis of maximum activating samples, but we use a different method as described in section \ref{sec:sampling}.

To run this method for a particular feature $f\in\mathcal{F}$, 
we need a set of highly activating samples $\mathcal S\subseteq \mathcal T$.
For sample $s\in\mathcal S$, we identify a \emph{pivot token} $e$, 
which is the most activating token in the sample, 
and perform 3 steps: pruning, saliencey detection, and augmentation.

\subsection{Pruning}
For a token string $s\in\mathcal S$, and pivot token $e$, 
pruning consists of finding the smallest substring of $s$ that ends in $e$ 
and still sufficiently activates $f$.
What "sufficiently activates" means is a parameter of the method, 
but in the original paper it is defined as causing an activation at least
half of original activation.
This removes context that is irrelevant to the activation of $f$.
We call the pruned string $s'$

\subsection{Saliency detection}
Here we find the most important tokens in the pruned string $s'$.
This is done by replacing each token in the pruned string 
with a padding token and finding the change in activation.
If the change is large, the token is important.
How large the change needs to be is another parameter of the model.
Once this step is done, we have a set $B$ of important tokens in $s'$.

\subsection{Augmentation}
Given a pruned string $s'$ and a set of important tokens $B$ in that string, 
augmentation is the process of finding nearby nearby strings 
that activate $f$ similarly to $s'$.
To do this, we replace each $b\in B$ with other "similar" tokens and 
see whether the resulting string activates $f$ sufficiently.
What counts as sufficient is yet another parameter, 
while "similar" tokens are found using a helper model 
(\verb|distilbert-base-uncased| in the original paper) 
that is asked to predict replacements for $b$ 
given the rest of $s'$ as context.
All alternative strings that activates $f$ sufficiently are stored.

\subsection{Graph building}
After performing the 3 previous steps on all strings in $\mathcal S$, 
We have a set $\mathcal S'\subseteq\mathcal T$ 
of pruned and augmented strings that all activate $f$ highly.
In order to make predictions we must build a model from these strings.
This is done by creating a trie $T$ 
by working backwards through each string.
The first nodes after the root of $T$ are the activating tokens 
in the strings of $S'$.
The rest of the nodes are the tokens in the strings of $S'$, 
so that each path from the root to a leaf represents a string in $S'$.
At the end of each of these paths through $T$, 
we add an end node storing the activation of $f$ on the represented string.
To predict the activation of $f$ on the last token of a new string $s$, 
we start from the root of $T$ and the last token of $s$.
We then traverse the trie, going backwards through $s$ 
following any node that matches the current token of $s$, 
with the special ignore tokens matching any token.
If we reach an end node, we return the activation stored there.
If at some point no node matches the current token of $s$, 
we return $0$.

This gives us a quantative measure of how good a model of the feature 
the graph is.
It also allows us to create a visual representation 
of the feature behaviour.
To do this, we create a new graph from $T$ 
where all ignore nodes are removed, 
and nodes representing the same token on the same layer are collapsed.
We refer to both this representation and the original trie $T$ 
as the \emph{feature graph}.

\section{Sparse autoencoders}
\label{sec:sae}
\input{chapters/theory/sae.tex}