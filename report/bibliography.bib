
@article{fry_towards_2024,
	title = {Towards {Multimodal} {Interpretability}: {Learning} {Sparse} {Interpretable} {Features} in {Vision} {Transformers}},
	shorttitle = {Towards {Multimodal} {Interpretability}},
	url = {https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2},
	abstract = {Executive Summary
In this post I present my results from training a Sparse Autoencoder (SAE) on a CLIP Vision Transformer (ViT) using the ImageNet-1k…},
	language = {en},
	urldate = {2024-05-31},
	author = {Fry, Hugo},
	month = apr,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/RALC3C54/towards-multimodal-interpretability-learning-sparse-2.html:text/html},
}

@article{zhaohaiyan_explainability_2024,
	title = {Explainability for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Explainability for {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3639372},
	doi = {10.1145/3639372},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural
language processing. However, their internal mechanisms are still unclear and this
lack of transparency poses unwanted risks for downstream applications. Therefore,
...},
	language = {EN},
	urldate = {2024-06-05},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {ZhaoHaiyan and ChenHanjie and YangFan and LiuNinghao and DengHuiqi and CaiHengyi and WangShuaiqiang and YinDawei and DuMengnan},
	month = feb,
	year = {2024},
	note = {Publisher: ACMPUB27New York, NY},
	file = {Full Text:/home/user/Zotero/storage/JT4Q2MV2/ZhaoHaiyan et al. - 2024 - Explainability for Large Language Models A Survey.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/9YBF5DWD/3639372.html:text/html},
}

@misc{rauker_toward_2023,
	title = {Toward {Transparent} {AI}: {A} {Survey} on {Interpreting} the {Inner} {Structures} of {Deep} {Neural} {Networks}},
	shorttitle = {Toward {Transparent} {AI}},
	url = {http://arxiv.org/abs/2207.13243},
	doi = {10.48550/arXiv.2207.13243},
	abstract = {The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Räuker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
	month = aug,
	year = {2023},
	note = {arXiv:2207.13243 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/JGFYG8BI/Räuker et al. - 2023 - Toward Transparent AI A Survey on Interpreting th.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/ARJ23ZLX/2207.html:text/html},
}

@article{elhage_softmax_2022,
	title = {Softmax {Linear} {Units}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Nanda, Neel and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Jones, Andy and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
	year = {2022},
}

@article{kissane_sparse_2024,
	title = {Sparse {Autoencoders} {Work} on {Attention} {Layer} {Outputs}},
	url = {https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs},
	abstract = {We replicate Anthropic's MLP Sparse Autoencoder (SAE) paper on attention outputs and it works well: the SAEs learn sparse, interpretable features, which gives us insight into what attention layers learn. We study the second attention layer of a two layer language model (with MLPs).
Specifically, rather than training our SAE on attn\_output, we train our SAE on “hook\_z” concatenated over all attention heads (aka the mixed values aka the attention outputs before a linear map - see notation here). This is valuable as we can see how much of each feature’s weights come from each head, which we believe is a promising direction to investigate attention head superposition, although we only briefly explore that in this work.
We open source our SAE, you can use it via this Colab notebook.
Shallow Dives: We do a shallow investigation to interpret each of the first 50 features. We estimate 82\% of non-dead features in our SAE are interpretable (24\% of the SAE features are dead).
See this feature interface to browse the first 50 features.
Deep dives: To verify our SAEs have learned something real, we zoom in on individual features for much more detailed investigations: the  “‘board’ is next by induction” feature, the local context feature of “in questions starting with ‘Which’”, and the more global context feature of “in texts about pets”.
We go beyond the techniques from the Anthropic paper, and investigate the circuits used to compute the features from earlier components, including analysing composition with an MLP0 SAE. 
We also investigate how the features are used downstream, and whether it's via MLP1 or the direct connection to the logits. 
Automation: We automatically detect and quantify a large “\{token\} is next by induction” feature family. This represents {\textasciitilde}5\% of the living features in the SAE.
Though the specific automation technique won't generalize to other feature families, this is notable, as if there are many “one feature per vocab token” families like this, we may need impractically wide SAEs for larger models.},
	language = {en},
	urldate = {2024-05-31},
	author = {Kissane, Connor and Krzyzanowski, Robert and Conmy, Arthur and Nanda, Neel},
	month = jan,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/9M4EDPGZ/sparse-autoencoders-work-on-attention-layer-outputs.html:text/html},
}

@misc{bushnaq_using_2024,
	title = {Using {Degeneracy} in the {Loss} {Landscape} for {Mechanistic} {Interpretability}},
	url = {http://arxiv.org/abs/2405.10927},
	doi = {10.48550/arXiv.2405.10927},
	abstract = {Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.},
	urldate = {2024-06-02},
	publisher = {arXiv},
	author = {Bushnaq, Lucius and Mendel, Jake and Heimersheim, Stefan and Braun, Dan and Goldowsky-Dill, Nicholas and Hänni, Kaarel and Wu, Cindy and Hobbhahn, Marius},
	month = may,
	year = {2024},
	note = {arXiv:2405.10927 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/M9YV28GW/Bushnaq et al. - 2024 - Using Degeneracy in the Loss Landscape for Mechani.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/W9QG4Q6T/2405.html:text/html},
}

@misc{anthropic_introducing_2024,
	title = {Introducing the next generation of {Claude}},
	url = {https://www.anthropic.com/news/claude-3-family},
	abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
	language = {en},
	urldate = {2024-05-30},
	author = {Anthropic},
	month = mar,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/BVVN2R2Q/claude-3-family.html:text/html},
}

@article{kissane_attention_2024,
	title = {Attention {SAEs} {Scale} to {GPT}-2 {Small}},
	url = {https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr/attention-saes-scale-to-gpt-2-small},
	abstract = {This is an interim report that we are currently building on. We hope this update + open sourcing our SAEs will be useful to related research occurrin…},
	language = {en},
	urldate = {2024-05-31},
	author = {Kissane, Connor and Krzyzanowski, Robert and Conmy, Arthur and Nanda, Neel},
	month = feb,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/JJPS7KH9/attention-saes-scale-to-gpt-2-small.html:text/html},
}

@article{krzyzanowski_we_2024,
	title = {We {Inspected} {Every} {Head} {In} {GPT}-2 {Small} using {SAEs} {So} {You} {Don}’t {Have} {To}},
	url = {https://www.alignmentforum.org/posts/xmegeW5mqiBsvoaim/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don},
	abstract = {This is an interim report that we are currently building on. We hope this update will be useful to related research occurring in parallel. Produced a…},
	language = {en},
	urldate = {2024-05-29},
	author = {Krzyzanowski, Robert and Kissane, Connor and Conmy, Arthur and Nanda, Neel},
	month = mar,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/LVHE8TG5/we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don.html:text/html},
}

@article{huben_research_2024,
	title = {Research {Report}: {Sparse} {Autoencoders} find only 9/180 board state features in {OthelloGPT}},
	shorttitle = {Research {Report}},
	url = {https://www.lesswrong.com/posts/BduCMgmjJnCtc7jKc/research-report-sparse-autoencoders-find-only-9-180-board},
	abstract = {[3/7 Edit: I have rephrased the bolded claims in the abstract per this comment from Joseph Bloom, hopefully improving the heat-to-light ratio.  …},
	language = {en},
	urldate = {2024-05-29},
	author = {Huben, Robert},
	month = mar,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/8F5UHS7M/research-report-sparse-autoencoders-find-only-9-180-board.html:text/html},
}

@article{millidge_singular_2022,
	title = {The {Singular} {Value} {Decompositions} of {Transformer} {Weight} {Matrices} are {Highly} {Interpretable}},
	url = {https://www.alignmentforum.org/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight},
	abstract = {If we take the SVD of the weight matrices of the OV circuit and of MLP layers of GPT models, and project them to token embedding space, we notice this results in highly interpretable semantic clusters. This means that the network learns to align the principal directions of each MLP weight matrix or attention head to read from or write to semantically interpretable directions in the residual stream.

We can use this to both improve our understanding of transformer language models and edit their representations. We use this finding to design both a natural language query locator, where you can write a set of natural language concepts and find all weight directions in the network which correspond to it, and also to edit the network's representations by deleting specific singular vectors, which results in relatively large effects on the logits related to the semantics of that vector and relatively small effects on semantically different clusters},
	language = {en},
	urldate = {2024-06-01},
	author = {Millidge, Beren and Black, Sid},
	month = nov,
	year = {2022},
	file = {Snapshot:/home/user/Zotero/storage/HXD7QF3V/the-singular-value-decompositions-of-transformer-weight.html:text/html},
}

@misc{peebles_hessian_2020,
	title = {The {Hessian} {Penalty}: {A} {Weak} {Prior} for {Unsupervised} {Disentanglement}},
	shorttitle = {The {Hessian} {Penalty}},
	url = {http://arxiv.org/abs/2008.10599},
	doi = {10.48550/arXiv.2008.10599},
	abstract = {Existing disentanglement methods for deep generative models rely on hand-picked priors and complex encoder-based architectures. In this paper, we propose the Hessian Penalty, a simple regularization term that encourages the Hessian of a generative model with respect to its input to be diagonal. We introduce a model-agnostic, unbiased stochastic approximation of this term based on Hutchinson's estimator to compute it efficiently during training. Our method can be applied to a wide range of deep generators with just a few lines of code. We show that training with the Hessian Penalty often causes axis-aligned disentanglement to emerge in latent space when applied to ProGAN on several datasets. Additionally, we use our regularization term to identify interpretable directions in BigGAN's latent space in an unsupervised fashion. Finally, we provide empirical evidence that the Hessian Penalty encourages substantial shrinkage when applied to over-parameterized latent spaces.},
	urldate = {2024-06-02},
	publisher = {arXiv},
	author = {Peebles, William and Peebles, John and Zhu, Jun-Yan and Efros, Alexei and Torralba, Antonio},
	month = aug,
	year = {2020},
	note = {arXiv:2008.10599 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/MJUGB2GQ/Peebles et al. - 2020 - The Hessian Penalty A Weak Prior for Unsupervised.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/IMMU7VVG/2008.html:text/html},
}

@misc{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	doi = {10.48550/arXiv.1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	urldate = {2024-06-02},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv:1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/YRAIDM73/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/9V7NBPE5/1206.html:text/html},
}

@misc{kim_disentangling_2019,
	title = {Disentangling by {Factorising}},
	url = {http://arxiv.org/abs/1802.05983},
	doi = {10.48550/arXiv.1802.05983},
	abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \${\textbackslash}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
	urldate = {2024-06-02},
	publisher = {arXiv},
	author = {Kim, Hyunjik and Mnih, Andriy},
	month = jul,
	year = {2019},
	note = {arXiv:1802.05983 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/QL4RKXVT/Kim and Mnih - 2019 - Disentangling by Factorising.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/LJVUSRRU/1802.html:text/html},
}

@misc{schneider_explaining_2021,
	title = {Explaining {Neural} {Networks} by {Decoding} {Layer} {Activations}},
	url = {http://arxiv.org/abs/2005.13630},
	doi = {10.48550/arXiv.2005.13630},
	abstract = {We present a `CLAssifier-DECoder' architecture ({\textbackslash}emph\{ClaDec\}) which facilitates the comprehension of the output of an arbitrary layer in a neural network (NN). It uses a decoder to transform the non-interpretable representation of the given layer to a representation that is more similar to the domain a human is familiar with. In an image recognition problem, one can recognize what information is represented by a layer by contrasting reconstructed images of {\textbackslash}emph\{ClaDec\} with those of a conventional auto-encoder(AE) serving as reference. We also extend {\textbackslash}emph\{ClaDec\} to allow the trade-off between human interpretability and fidelity. We evaluate our approach for image classification using Convolutional NNs. We show that reconstructed visualizations using encodings from a classifier capture more relevant information for classification than conventional AEs. Relevant code is available at {\textbackslash}url\{https://github.com/JohnTailor/ClaDec\}},
	urldate = {2024-06-02},
	publisher = {arXiv},
	author = {Schneider, Johannes and Vlachos, Michalis},
	month = feb,
	year = {2021},
	note = {arXiv:2005.13630 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/DW9NI3ZW/Schneider and Vlachos - 2021 - Explaining Neural Networks by Decoding Layer Activ.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/B94232YR/2005.html:text/html},
}

@misc{tamkin_codebook_2023,
	title = {Codebook {Features}: {Sparse} and {Discrete} {Interpretability} for {Neural} {Networks}},
	shorttitle = {Codebook {Features}},
	url = {http://arxiv.org/abs/2310.17230},
	doi = {10.48550/arXiv.2310.17230},
	abstract = {Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on several different datasets. First, we explore a finite state machine dataset with far more hidden states than neurons. In this setting, our approach overcomes the superposition problem by assigning states to distinct codes, and we find that we can make the neural network behave as if it is in a different state by activating the code for that state. Second, we train Transformer language models with up to 410M parameters on two natural language datasets. We identify codes in these models representing diverse, disentangled concepts (ranging from negative emotions to months of the year) and find that we can guide the model to generate different topics by activating the appropriate codes during inference. Overall, codebook features appear to be a promising unit of analysis and control for neural networks and interpretability. Our codebase and models are open-sourced at https://github.com/taufeeque9/codebook-features.},
	urldate = {2024-06-01},
	publisher = {arXiv},
	author = {Tamkin, Alex and Taufeeque, Mohammad and Goodman, Noah D.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.17230 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/47BZLQ76/Tamkin et al. - 2023 - Codebook Features Sparse and Discrete Interpretab.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/QHGTTXYX/2310.html:text/html},
}

@misc{neel_nanda_actually_2023,
	title = {Actually, {Othello}-{GPT} {Has} {A} {Linear} {Emergent} {World} {Representation}},
	url = {https://www.neelnanda.io/mechanistic-interpretability/othello},
	abstract = {A write up of work extending and building on the paper Emergent World Representations},
	language = {en-US},
	urldate = {2024-06-01},
	author = {Neel Nanda},
	month = mar,
	year = {2023},
	file = {Snapshot:/home/user/Zotero/storage/EH24MZCB/othello.html:text/html},
}

@misc{hazineh_linear_2023,
	title = {Linear {Latent} {World} {Models} in {Simple} {Transformers}: {A} {Case} {Study} on {Othello}-{GPT}},
	shorttitle = {Linear {Latent} {World} {Models} in {Simple} {Transformers}},
	url = {http://arxiv.org/abs/2310.07582},
	doi = {10.48550/arXiv.2310.07582},
	abstract = {Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.},
	urldate = {2024-06-01},
	publisher = {arXiv},
	author = {Hazineh, Dean S. and Zhang, Zechen and Chiu, Jeffery},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07582 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/SBTJJL3M/Hazineh et al. - 2023 - Linear Latent World Models in Simple Transformers.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/CXTP5YHR/2310.html:text/html},
}

@misc{li_emergent_2023,
	title = {Emergent {World} {Representations}: {Exploring} a {Sequence} {Model} {Trained} on a {Synthetic} {Task}},
	shorttitle = {Emergent {World} {Representations}},
	url = {http://arxiv.org/abs/2210.13382},
	doi = {10.48550/arXiv.2210.13382},
	abstract = {Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create "latent saliency maps" that can help explain predictions in human terms.},
	urldate = {2024-06-01},
	publisher = {arXiv},
	author = {Li, Kenneth and Hopkins, Aspen K. and Bau, David and Viégas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
	month = feb,
	year = {2023},
	note = {arXiv:2210.13382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/ACZPBJ8D/Li et al. - 2023 - Emergent World Representations Exploring a Sequen.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/47FPHJ99/2210.html:text/html},
}

@article{bills_language_2023,
	title = {Language models can explain neurons in language models},
	journal = {URL https://openaipublic.blob.core.windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)},
	author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
	year = {2023},
}

@article{nostalgebraist_interpreting_2020,
	title = {interpreting {GPT}: the logit lens},
	shorttitle = {interpreting {GPT}},
	url = {https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
	abstract = {This post relates an observation I've made in my work with GPT-2, which I have not seen made elsewhere. …},
	language = {en},
	urldate = {2024-05-31},
	author = {nostalgebraist},
	month = aug,
	year = {2020},
	file = {Snapshot:/home/user/Zotero/storage/MIXF9SZP/interpreting-gpt-the-logit-lens.html:text/html},
}

@misc{belrose_eliciting_2023,
	title = {Eliciting {Latent} {Predictions} from {Transformers} with the {Tuned} {Lens}},
	url = {http://arxiv.org/abs/2303.08112},
	doi = {10.48550/arXiv.2303.08112},
	abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the {\textbackslash}emph\{tuned lens\}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
	month = nov,
	year = {2023},
	note = {arXiv:2303.08112 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/IUGATVYR/Belrose et al. - 2023 - Eliciting Latent Predictions from Transformers wit.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/6XCJJ83B/2303.html:text/html},
}

@misc{dao_adversarial_2023,
	title = {An {Adversarial} {Example} for {Direct} {Logit} {Attribution}: {Memory} {Management} in gelu-4l},
	shorttitle = {An {Adversarial} {Example} for {Direct} {Logit} {Attribution}},
	url = {http://arxiv.org/abs/2310.07325},
	doi = {10.48550/arXiv.2310.07325},
	abstract = {How do language models deal with the limited bandwidth of the residual stream? Prior work has suggested that some attention heads and MLP layers may perform a "memory management" role. That is, clearing residual stream directions set by earlier layers by reading in information and writing out the negative version. In this work, we present concrete evidence for this phenomenon in a 4-layer transformer. We identify several heads in layer 2 that consistently remove the output of a single layer 0 head. We then verify that this erasure causally depends on the original written direction. We further demonstrate that direct logit attribution (DLA) suggests that writing and erasing heads directly contribute to predictions, when in fact their effects cancel out. Then we present adversarial prompts for which this effect is particularly salient. These findings reveal that memory management can make DLA results misleading. Accordingly, we make concrete recommendations for circuit analysis to prevent interpretability illusions.},
	urldate = {2024-05-31},
	publisher = {arXiv},
	author = {Dao, James and Lau, Yeu-Tong and Rager, Can and Janiak, Jett},
	month = nov,
	year = {2023},
	note = {arXiv:2310.07325 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/VVJ78KJJ/Dao et al. - 2023 - An Adversarial Example for Direct Logit Attributio.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/7TVSX728/2310.html:text/html},
}

@article{casper_eis_2024,
	title = {{EIS} {XIII}: {Reflections} on {Anthropic}’s {SAE} {Research} {Circa} {May} 2024},
	shorttitle = {{EIS} {XIII}},
	url = {https://www.alignmentforum.org/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may},
	abstract = {Part 13 of 12 in the Engineer’s Interpretability Sequence. …},
	language = {en},
	urldate = {2024-05-31},
	author = {Casper, Stephen},
	month = may,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/T4IIZNAB/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may.html:text/html},
}

@article{templeton_scaling_2024,
	title = {Scaling {Monosemanticity}: {Extracting} {Interpretable} {Features} from {Claude} 3 {Sonnet}},
	url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
	journal = {Transformer Circuits Thread},
	author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
	year = {2024},
}

@article{marks_discriminating_2024,
	title = {Discriminating {Behaviorally} {Identical} {Classifiers}: a model problem for applying interpretability to scalable oversight},
	shorttitle = {Discriminating {Behaviorally} {Identical} {Classifiers}},
	url = {https://www.alignmentforum.org/posts/s7uD3tzHMvD868ehr/discriminating-behaviorally-identical-classifiers-a-model},
	abstract = {In a new preprint, Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models, my coauthors and I introduce a te…},
	language = {en},
	urldate = {2024-05-30},
	author = {Marks, Sam},
	month = apr,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/KTC49Q3A/discriminating-behaviorally-identical-classifiers-a-model.html:text/html},
}

@misc{marks_sparse_2024,
	title = {Sparse {Feature} {Circuits}: {Discovering} and {Editing} {Interpretable} {Causal} {Graphs} in {Language} {Models}},
	shorttitle = {Sparse {Feature} {Circuits}},
	url = {http://arxiv.org/abs/2403.19647},
	doi = {10.48550/arXiv.2403.19647},
	abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19647 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/C4NGQC5Y/Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/6TXVPXGR/2403.html:text/html},
}

@misc{makelov_towards_2024,
	title = {Towards {Principled} {Evaluations} of {Sparse} {Autoencoders} for {Interpretability} and {Control}},
	url = {http://arxiv.org/abs/2405.08366},
	doi = {10.48550/arXiv.2405.08366},
	abstract = {Disentangling model activations into meaningful features is a central problem in interpretability. However, the absence of ground-truth for these features in realistic scenarios makes validating recent approaches, such as sparse dictionary learning, elusive. To address this challenge, we propose a framework for evaluating feature dictionaries in the context of specific tasks, by comparing them against {\textbackslash}emph\{supervised\} feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes. We apply this framework to the indirect object identification (IOI) task using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets. We find that these SAEs capture interpretable features for the IOI task, but they are less successful than supervised features in controlling the model. Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller, less interpretable features). We hope that our framework will provide a useful step towards more objective and grounded evaluations of sparse dictionary learning methods.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Makelov, Aleksandar and Lange, George and Nanda, Neel},
	month = may,
	year = {2024},
	note = {arXiv:2405.08366 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/E8T4DFET/Makelov et al. - 2024 - Towards Principled Evaluations of Sparse Autoencod.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/963U7QQV/2405.html:text/html},
}

@misc{black_interpreting_2022,
	title = {Interpreting {Neural} {Networks} through the {Polytope} {Lens}},
	url = {http://arxiv.org/abs/2211.12312},
	doi = {10.48550/arXiv.2211.12312},
	abstract = {Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description: directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a specific feature to a neural unit. In order to find a basic unit of description that does not suffer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the polytope lens. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classifiers and language models. Specifically, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reflect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Black, Sid and Sharkey, Lee and Grinsztajn, Leo and Winsor, Eric and Braun, Dan and Merizian, Jacob and Parker, Kip and Guevara, Carlos Ramón and Millidge, Beren and Alfour, Gabriel and Leahy, Connor},
	month = nov,
	year = {2022},
	note = {arXiv:2211.12312 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/I6RWZ3BY/Black et al. - 2022 - Interpreting Neural Networks through the Polytope .pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/LDJAQRUU/2211.html:text/html},
}

@misc{bereska_mechanistic_2024,
	title = {Mechanistic {Interpretability} for {AI} {Safety} -- {A} {Review}},
	url = {http://arxiv.org/abs/2404.14082},
	doi = {10.48550/arXiv.2404.14082},
	abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Bereska, Leonard and Gavves, Efstratios},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14082 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/LLM3K49F/Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety -- A Re.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/7QJ9TRIE/2404.html:text/html},
}

@article{taggart_prolu_2024,
	title = {{ProLU}: {A} {Nonlinearity} for {Sparse} {Autoencoders}},
	shorttitle = {{ProLU}},
	url = {https://www.alignmentforum.org/posts/HEpufTdakGTTKgoYF/prolu-a-nonlinearity-for-sparse-autoencoders},
	abstract = {Abstract
This paper presents ProLU, an alternative to ReLU for the activation function in sparse autoencoders that produces a pareto improvement over…},
	language = {en},
	urldate = {2024-05-30},
	author = {Taggart, Glen},
	month = apr,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/NJGMVKPI/prolu-a-nonlinearity-for-sparse-autoencoders.html:text/html},
}

@article{wright_addressing_2024,
	title = {Addressing {Feature} {Suppression} in {SAEs}},
	url = {https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes},
	abstract = {Produced as part of the ML Alignment Theory Scholars Program - Winter 2023-24 Cohort as part of Lee Sharkey's stream. …},
	language = {en},
	urldate = {2024-05-29},
	author = {Wright, Benjamin and Sharkey, Lee},
	month = feb,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/8RG94LGD/addressing-feature-suppression-in-saes.html:text/html},
}

@misc{rajamanoharan_improving_2024,
	title = {Improving {Dictionary} {Learning} with {Gated} {Sparse} {Autoencoders}},
	url = {http://arxiv.org/abs/2404.16014},
	doi = {10.48550/arXiv.2404.16014},
	abstract = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kramár, János and Shah, Rohin and Nanda, Neel},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16014 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/6IF5FWLD/Rajamanoharan et al. - 2024 - Improving Dictionary Learning with Gated Sparse Au.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/CGSN5Q7N/2404.html:text/html},
}

@article{conmy_my_2023,
	title = {My best guess at the important tricks for training {1L} {SAEs}},
	url = {https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes},
	abstract = {TL;DR: this quickly-written post gives a list of my guesses of the most important parts of training a Sparse Autoencoder on a 1L Transformer, with op…},
	language = {en},
	urldate = {2024-05-30},
	author = {Conmy, Arthur},
	month = dec,
	year = {2023},
	file = {Snapshot:/home/user/Zotero/storage/S7K3VH4S/my-best-guess-at-the-important-tricks-for-training-1l-saes.html:text/html},
}

@article{olshausen_sparse_1997,
	title = {Sparse coding with an overcomplete basis set: a strategy employed by {V1}?},
	volume = {37},
	issn = {0042-6989},
	shorttitle = {Sparse coding with an overcomplete basis set},
	doi = {10.1016/s0042-6989(97)00169-7},
	abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
	language = {eng},
	number = {23},
	journal = {Vision Research},
	author = {Olshausen, B. A. and Field, D. J.},
	month = dec,
	year = {1997},
	pmid = {9425546},
	keywords = {Algorithms, Animals, Coding, Gabor-wavelet, Mammals, Models, Psychological, Natural images, V1, Visual Cortex, Visual Perception},
	pages = {3311--3325},
	file = {ScienceDirect Snapshot:/home/user/Zotero/storage/QM6P828B/S0042698997001697.html:text/html},
}

@article{sharkey_interim_2022,
	title = {[{Interim} research report] {Taking} features out of superposition with sparse autoencoders},
	url = {https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition},
	abstract = {We're thankful for helpful comments from Trenton Bricken, Eric Winsor, Noa Nabeshima, and Sid Black.  …},
	language = {en},
	urldate = {2024-05-30},
	author = {Sharkey, Lee and Braun, Dan and Millidge, Beren},
	month = dec,
	year = {2022},
	file = {Snapshot:/home/user/Zotero/storage/X4I9VTV3/interim-research-report-taking-features-out-of-superposition.html:text/html},
}

@misc{gurnee_universal_2024,
	title = {Universal {Neurons} in {GPT2} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.12181},
	abstract = {A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms. In other words, are neural mechanisms universal across different models? In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable. In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs. We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families. We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12181 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {2401.pdf:/home/user/Zotero/storage/VJ6EWDT6/2401.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/CK3JFS6E/2401.html:text/html},
}

@misc{park_linear_2023,
	title = {The {Linear} {Representation} {Hypothesis} and the {Geometry} of {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2311.03658v1},
	abstract = {Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/home/user/Zotero/storage/4JTNAQH3/Park et al. - 2023 - The Linear Representation Hypothesis and the Geome.pdf:application/pdf},
}

@misc{bushnaq_local_2024,
	title = {The {Local} {Interaction} {Basis}: {Identifying} {Computationally}-{Relevant} and {Sparsely} {Interacting} {Features} in {Neural} {Networks}},
	shorttitle = {The {Local} {Interaction} {Basis}},
	url = {https://arxiv.org/abs/2405.10928v2},
	abstract = {Mechanistic interpretability aims to understand the behavior of neural networks by reverse-engineering their internal computations. However, current methods struggle to find clear interpretations of neural network activations because a decomposition of activations into computational features is missing. Individual neurons or model components do not cleanly correspond to distinct features or functions. We present a novel interpretability method that aims to overcome this limitation by transforming the activations of the network into a new basis - the Local Interaction Basis (LIB). LIB aims to identify computational features by removing irrelevant activations and interactions. Our method drops irrelevant activation directions and aligns the basis with the singular vectors of the Jacobian matrix between adjacent layers. It also scales features based on their importance for downstream computation, producing an interaction graph that shows all computationally-relevant features and interactions in a model. We evaluate the effectiveness of LIB on modular addition and CIFAR-10 models, finding that it identifies more computationally-relevant features that interact more sparsely, compared to principal component analysis. However, LIB does not yield substantial improvements in interpretability or interaction sparsity when applied to language models. We conclude that LIB is a promising theory-driven approach for analyzing neural networks, but in its current form is not applicable to large language models.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Bushnaq, Lucius and Heimersheim, Stefan and Goldowsky-Dill, Nicholas and Braun, Dan and Mendel, Jake and Hänni, Kaarel and Griffin, Avery and Stöhler, Jörn and Wache, Magdalena and Hobbhahn, Marius},
	month = may,
	year = {2024},
	file = {Full Text PDF:/home/user/Zotero/storage/8YPM95AE/Bushnaq et al. - 2024 - The Local Interaction Basis Identifying Computati.pdf:application/pdf},
}

@misc{jorgensen_improving_2023,
	title = {Improving {Activation} {Steering} in {Language} {Models} with {Mean}-{Centring}},
	url = {https://arxiv.org/abs/2312.03813v1},
	abstract = {Recent work in activation steering has demonstrated the potential to better control the outputs of Large Language Models (LLMs), but it involves finding steering vectors. This is difficult because engineers do not typically know how features are represented in these models. We seek to address this issue by applying the idea of mean-centring to steering vectors. We find that taking the average of activations associated with a target dataset, and then subtracting the mean of all training activations, results in effective steering vectors. We test this method on a variety of models on natural language tasks by steering away from generating toxic text, and steering the completion of a story towards a target genre. We also apply mean-centring to extract function vectors, more effectively triggering the execution of a range of natural language tasks by a significant margin (compared to previous baselines). This suggests that mean-centring can be used to easily improve the effectiveness of activation steering in a wide range of contexts.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Jorgensen, Ole and Cope, Dylan and Schoots, Nandi and Shanahan, Murray},
	month = dec,
	year = {2023},
	file = {Full Text PDF:/home/user/Zotero/storage/7PK92NAM/Jorgensen et al. - 2023 - Improving Activation Steering in Language Models w.pdf:application/pdf},
}

@inproceedings{wang_interpretability_2022,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 {Small}},
	shorttitle = {Interpretability in the {Wild}},
	url = {https://openreview.net/forum?id=NpsVSN6o4ul},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria - faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	language = {en},
	urldate = {2024-05-29},
	author = {Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/home/user/Zotero/storage/8HWH7FI7/Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indire.pdf:application/pdf},
}

@misc{deng_measuring_2023,
	title = {Measuring {Feature} {Sparsity} in {Language} {Models}},
	url = {https://arxiv.org/abs/2310.07837v2},
	abstract = {Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Deng, Mingyang and Tao, Lucas and Benton, Joe},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/home/user/Zotero/storage/IPYA3B8I/Deng et al. - 2023 - Measuring Feature Sparsity in Language Models.pdf:application/pdf},
}

@misc{engels_not_2024,
	title = {Not {All} {Language} {Model} {Features} {Are} {Linear}},
	url = {https://arxiv.org/abs/2405.14860v1},
	abstract = {Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Finally, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we find further circular representations by breaking down the hidden states for these tasks into interpretable components.},
	language = {en},
	urldate = {2024-05-29},
	journal = {arXiv.org},
	author = {Engels, Joshua and Liao, Isaac and Michaud, Eric J. and Gurnee, Wes and Tegmark, Max},
	month = may,
	year = {2024},
	file = {Full Text PDF:/home/user/Zotero/storage/F4GD9DJD/Engels et al. - 2024 - Not All Language Model Features Are Linear.pdf:application/pdf},
}

@misc{bolukbasi_interpretability_2021,
	title = {An {Interpretability} {Illusion} for {BERT}},
	url = {http://arxiv.org/abs/2104.07143},
	doi = {10.48550/arXiv.2104.07143},
	abstract = {We describe an "interpretability illusion" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.},
	urldate = {2024-02-12},
	publisher = {arXiv},
	author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Viégas, Fernanda and Wattenberg, Martin},
	month = apr,
	year = {2021},
	note = {arXiv:2104.07143 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/X3L3IRUW/Bolukbasi et al. - 2021 - An Interpretability Illusion for BERT.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/GKKVL2M3/2104.html:text/html},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/GGYGXFZ5/Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/MPIW2KKM/2309.html:text/html},
}

@article{bloom_understanding_2024,
	title = {Understanding {SAE} {Features} with the {Logit} {Lens}},
	url = {https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-with-the-logit-lens},
	abstract = {This work was produced as part of the ML Alignment \& Theory Scholars Program - Winter 2023-24 Cohort, with support from Neel Nanda and Arthur Conmy.…},
	language = {en},
	urldate = {2024-05-29},
	author = {Bloom, Joseph and Lin, Johnny},
	month = mar,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/NY2F6V8Y/understanding-sae-features-with-the-logit-lens.html:text/html},
}

@article{lin_announcing_2024,
	title = {Announcing {Neuronpedia}: {Platform} for accelerating research into {Sparse} {Autoencoders}},
	shorttitle = {Announcing {Neuronpedia}},
	url = {https://www.alignmentforum.org/posts/BaEQoxHhWPrkinmxd/announcing-neuronpedia-platform-for-accelerating-research},
	abstract = {This posts assumes basic familiarity with Sparse Autoencoders. For those unfamiliar with this technique, we highly recommend the introductory section…},
	language = {en},
	urldate = {2024-05-29},
	author = {Lin, Johnny and Bloom, Joseph},
	month = mar,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/ZL5WVYU4/announcing-neuronpedia-platform-for-accelerating-research.html:text/html},
}

@article{vaintrob_toward_2024,
	title = {Toward {A} {Mathematical} {Framework} for {Computation} in {Superposition}},
	url = {https://www.alignmentforum.org/posts/2roZtSr5TGmLjXMnT/toward-a-mathematical-framework-for-computation-in},
	abstract = {Author order randomized. Authors contributed roughly equally — see attribution section for details. …},
	language = {en},
	urldate = {2024-05-29},
	author = {Vaintrob, Dmitry and Mendel, Jake and Kaarel},
	month = jan,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/THGKRULI/toward-a-mathematical-framework-for-computation-in.html:text/html},
}

@article{riggs_finding_2023,
	title = {Finding {Sparse} {Linear} {Connections} between {Features} in {LLMs}},
	url = {https://www.alignmentforum.org/posts/7fxusXdkMNmAhkAfc/finding-sparse-linear-connections-between-features-in-llms},
	abstract = {TL;DR: We use SGD to find sparse connections between features; additionally a large fraction of features between the residual stream \& MLP can be mod…},
	language = {en},
	urldate = {2024-05-29},
	author = {Riggs, Logan and Mitchell, Sam and Eccentricity},
	month = dec,
	year = {2023},
	file = {Snapshot:/home/user/Zotero/storage/XYNRDPBS/finding-sparse-linear-connections-between-features-in-llms.html:text/html},
}

@article{riggs_improving_2024,
	title = {Improving {SAE}'s by {Sqrt}()-ing {L1} \& {Removing} {Lowest} {Activating} {Features}},
	url = {https://www.alignmentforum.org/posts/YiGs8qJ8aNBgwt2YN/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest},
	abstract = {TL;DR
We achieve better SAE performance by: …},
	language = {en},
	urldate = {2024-05-29},
	author = {Riggs, Logan and Brinkmann, Jannik},
	month = mar,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/DFF8YG2W/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest.html:text/html},
}

@article{olsson_-context_2022,
	title = {In-context {Learning} and {Induction} {Heads}},
	journal = {Transformer Circuits Thread},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2022},
}

@article{elhage_mathematical_2021,
	title = {A {Mathematical} {Framework} for {Transformer} {Circuits}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2021},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/R6C7FF8F/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/G6B5WF34/1706.html:text/html},
}

@misc{hao_self-attention_2021,
	title = {Self-{Attention} {Attribution}: {Interpreting} {Information} {Interactions} {Inside} {Transformer}},
	shorttitle = {Self-{Attention} {Attribution}},
	url = {http://arxiv.org/abs/2004.11207},
	doi = {10.48550/arXiv.2004.11207},
	abstract = {The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
	month = feb,
	year = {2021},
	note = {arXiv:2004.11207 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/IBJ5ZJMU/Hao et al. - 2021 - Self-Attention Attribution Interpreting Informati.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/6UC5SYBS/2004.html:text/html},
}

@misc{conmy_towards_2023,
	title = {Towards {Automated} {Circuit} {Discovery} for {Mechanistic} {Interpretability}},
	url = {http://arxiv.org/abs/2304.14997},
	doi = {10.48550/arXiv.2304.14997},
	abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.},
	urldate = {2024-02-12},
	publisher = {arXiv},
	author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
	month = oct,
	year = {2023},
	note = {arXiv:2304.14997 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/2FWBXADC/Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/MNJ469VS/2304.html:text/html},
}

@article{elhage_toy_2022,
	title = {Toy {Models} of {Superposition}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
	year = {2022},
}

@article{bricken_towards_2023,
	title = {Towards {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}},
	journal = {Transformer Circuits Thread},
	author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
	year = {2023},
}

@misc{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	doi = {10.48550/arXiv.2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv:2002.12327 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/9H3J65NM/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/44E2CYUZ/2002.html:text/html},
}

@misc{garde_deepdecipher_2023,
	title = {{DeepDecipher}: {Accessing} and {Investigating} {Neuron} {Activation} in {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Licence (CC-BY-NC-ND)},
	shorttitle = {{DeepDecipher}},
	url = {http://arxiv.org/abs/2310.01870},
	doi = {10.48550/arXiv.2310.01870},
	abstract = {As large language models (LLMs) become more capable, there is an urgent need for interpretable and transparent tools. Current methods are difficult to implement, and accessible tools to analyze model internals are lacking. To bridge this gap, we present DeepDecipher - an API and interface for probing neurons in transformer models' MLP layers. DeepDecipher makes the outputs of advanced interpretability techniques for LLMs readily available. The easy-to-use interface also makes inspecting these complex models more intuitive. This paper outlines DeepDecipher's design and capabilities. We demonstrate how to analyze neurons, compare models, and gain insights into model behavior. For example, we contrast DeepDecipher's functionality with similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher enables efficient, scalable analysis of LLMs. By granting access to state-of-the-art interpretability methods, DeepDecipher makes LLMs more transparent, trustworthy, and safe. Researchers, engineers, and developers can quickly diagnose issues, audit systems, and advance the field.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Garde, Albert and Kran, Esben and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2310.01870 [cs]},
	keywords = {Computer Science - Machine Learning, 68T50 (Primary) 68T05 (Secondary), I.2.7},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/3RIPKI9H/Garde et al. - 2023 - DeepDecipher Accessing and Investigating Neuron A.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/HPGFUTED/2310.html:text/html},
}

@misc{foote_neuron_2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/EHVYIWBD/Foote et al. - 2023 - Neuron to Graph Interpreting Language Model Neuro.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/XLRS23N4/2305.html:text/html},
}

@misc{braun_identifying_2024,
	title = {Identifying {Functionally} {Important} {Features} with {End}-to-{End} {Sparse} {Dictionary} {Learning}},
	url = {http://arxiv.org/abs/2405.12241},
	doi = {10.48550/arXiv.2405.12241},
	abstract = {Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e\_sae},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Braun, Dan and Taylor, Jordan and Goldowsky-Dill, Nicholas and Sharkey, Lee},
	month = may,
	year = {2024},
	note = {arXiv:2405.12241 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/9DQYK7FL/Braun et al. - 2024 - Identifying Functionally Important Features with E.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/3MHUDCBX/2405.html:text/html},
}

@misc{gao_scaling_2024,
	title = {Scaling and evaluating sparse autoencoders},
	url = {http://arxiv.org/abs/2406.04093},
	doi = {10.48550/arXiv.2406.04093},
	abstract = {Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Gao, Leo and la Tour, Tom Dupré and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/AXN3PP3R/Gao et al. - 2024 - Scaling and evaluating sparse autoencoders.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/J58KAJTW/2406.html:text/html},
}

@inproceedings{katz_visit_2023,
	title = {{VISIT}: {Visualizing} and {Interpreting} the {Semantic} {Information} {Flow} of {Transformers}},
	shorttitle = {{VISIT}},
	url = {https://openreview.net/forum?id=7O9bTjLgTQ},
	abstract = {Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on our discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that can reflect the models' internal processing, uncovering the contribution of each component to the models' final prediction. Our visualization also unveils new insights about the role of layer norms as semantic filters that influence the models' output, and about neurons that are always activated during forward passes and act as regularization vectors.},
	language = {en},
	urldate = {2024-06-12},
	author = {Katz, Shahar and Belinkov, Yonatan},
	month = dec,
	year = {2023},
	file = {Full Text PDF:/home/user/Zotero/storage/9ZBT5EAY/Katz and Belinkov - 2023 - VISIT Visualizing and Interpreting the Semantic I.pdf:application/pdf},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/J8SG6S9T/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/9VWD3B8V/2303.html:text/html},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/home/user/Zotero/storage/WJ5YFB9I/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@misc{nanda_neelnandasparse_autoencoder_nodate,
	title = {{NeelNanda}/sparse\_autoencoder · {Hugging} {Face}},
	url = {https://huggingface.co/NeelNanda/sparse_autoencoder},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-06-13},
	author = {Nanda, Neel},
	file = {Snapshot:/home/user/Zotero/storage/5GKTI527/sparse_autoencoder.html:text/html},
}

@article{quaisley_research_2024,
	title = {Research {Report}: {Alternative} sparsity methods for sparse autoencoders with {OthelloGPT}.},
	shorttitle = {Research {Report}},
	url = {https://www.lesswrong.com/posts/ignCBxbqWWPYCdCCx/research-report-alternative-sparsity-methods-for-sparse},
	abstract = {Abstract
Standard sparse autoencoder training uses an L1 sparsity loss term to induce sparsity in the hidden layer.  However, theoretical justificati…},
	language = {en},
	urldate = {2024-06-16},
	author = {Quaisley, Andrew},
	month = jun,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/M998VZK4/research-report-alternative-sparsity-methods-for-sparse.html:text/html},
}

@book{wright_high-dimensional_2022,
	title = {High-{Dimensional} {Data} {Analysis} with {Low}-{Dimensional} {Models}: {Principles}, {Computation}, and {Applications}},
	publisher = {Cambridge University Press},
	author = {Wright, John and Ma, Yi},
	year = {2022},
}

@misc{noauthor_statement_nodate,
	title = {Statement on {AI} {Risk} {\textbar} {CAIS}},
	url = {https://www.safe.ai/work/statement-on-ai-risk},
	abstract = {A statement jointly signed by a historic coalition of experts: “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.”},
	language = {en},
	urldate = {2024-06-18},
	file = {Snapshot:/home/user/Zotero/storage/BHEIUWU3/statement-on-ai-risk.html:text/html},
}

@misc{ngo_alignment_2024,
	title = {The {Alignment} {Problem} from a {Deep} {Learning} {Perspective}},
	url = {http://arxiv.org/abs/2209.00626},
	abstract = {In coming years or decades, artiﬁcial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that are in conﬂict (i.e., misaligned) with human interests. If trained like today’s most capable models, AGIs could learn to act deceptively to receive higher reward, learn misaligned internally-represented goals which generalize beyond their ﬁne-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difﬁcult to align and may appear aligned even when they are not. Finally, we brieﬂy outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and we review research directions aimed at preventing this outcome.},
	language = {en},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Ngo, Richard and Chan, Lawrence and Mindermann, Sören},
	month = mar,
	year = {2024},
	note = {arXiv:2209.00626 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Ngo et al. - 2024 - The Alignment Problem from a Deep Learning Perspec.pdf:/home/user/Zotero/storage/HJSXTN36/Ngo et al. - 2024 - The Alignment Problem from a Deep Learning Perspec.pdf:application/pdf},
}

@misc{hendrycks_overview_2023,
	title = {An {Overview} of {Catastrophic} {AI} {Risks}},
	url = {http://arxiv.org/abs/2306.12001},
	doi = {10.48550/arXiv.2306.12001},
	abstract = {Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
	month = oct,
	year = {2023},
	note = {arXiv:2306.12001 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/home/user/Zotero/storage/CHY93XY7/Hendrycks et al. - 2023 - An Overview of Catastrophic AI Risks.pdf:application/pdf;arXiv.org Snapshot:/home/user/Zotero/storage/QP7QVB8F/2306.html:text/html},
}

@misc{marsh_astral-shruff_2024,
	title = {astral-sh/ruff},
	copyright = {MIT},
	url = {https://github.com/astral-sh/ruff},
	abstract = {An extremely fast Python linter and code formatter, written in Rust.},
	urldate = {2024-06-18},
	publisher = {Astral},
	author = {Marsh, Charlie},
	month = jun,
	year = {2024},
	note = {original-date: 2022-08-09T17:17:44Z},
	keywords = {linter, pep8, python, python3, ruff, rust, rustpython, static-analysis, static-code-analysis, style-guide, styleguide},
}

@misc{wu_openaisparse_autoencoder_2024,
	title = {openai/sparse\_autoencoder},
	copyright = {MIT},
	url = {https://github.com/openai/sparse_autoencoder},
	urldate = {2024-06-18},
	publisher = {OpenAI},
	author = {Wu, Jeff},
	month = jun,
	year = {2024},
	note = {original-date: 2024-06-12T04:36:12Z},
}

@misc{cooney_ai-safety-foundationsparse_autoencoder_2024,
	title = {ai-safety-foundation/sparse\_autoencoder},
	copyright = {MIT},
	url = {https://github.com/ai-safety-foundation/sparse_autoencoder},
	abstract = {Sparse Autoencoder for Mechanistic Interpretability},
	urldate = {2024-06-18},
	publisher = {ai-safety-foundation},
	author = {Cooney, Alan},
	month = jun,
	year = {2024},
	note = {original-date: 2023-10-27T07:37:15Z},
}

@misc{bloom_jbloomaussaelens_2024,
	title = {{jbloomAus}/{SAELens}},
	copyright = {MIT},
	url = {https://github.com/jbloomAus/SAELens},
	abstract = {Training Sparse Autoencoders on Language Models},
	urldate = {2024-06-18},
	author = {Bloom, Joseph},
	month = jun,
	year = {2024},
	note = {original-date: 2023-11-29T10:37:55Z},
}

@misc{nanda_neelnandac4-code-20k_nodate,
	title = {{NeelNanda}/c4-code-20k · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/NeelNanda/c4-code-20k},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-06-18},
	author = {Nanda, Neel},
	file = {Snapshot:/home/user/Zotero/storage/5USLHKGR/c4-code-20k.html:text/html},
}

@misc{foote_apartresearchneuron2graph_2024,
	title = {apartresearch/{Neuron2Graph}},
	copyright = {Apache-2.0},
	url = {https://github.com/apartresearch/Neuron2Graph},
	abstract = {Tools for exploring Transformer neuron behaviour, including input pruning and diversification.},
	urldate = {2024-06-18},
	publisher = {apartresearch},
	author = {Foote, Alex},
	month = jun,
	year = {2024},
	note = {original-date: 2022-11-14T12:27:28Z},
}

@article{makelov_saes_2023,
	title = {{SAEs} {Discover} {Meaningful} {Features} in the {IOI} {Task}},
	url = {https://www.lesswrong.com/posts/zj3GKWAnhPgTARByB/saes-discover-meaningful-features-in-the-ioi-task},
	abstract = {TLDR: recently, we wrote a paper proposing several evaluations of SAEs against "ground-truth" features computed w/ supervision for a given task (in o…},
	language = {en},
	urldate = {2024-06-19},
	author = {Makelov, Alex and Lange, Georg and Nanda, Neel},
	month = dec,
	year = {2023},
	file = {Snapshot:/home/user/Zotero/storage/2LIHG4LU/saes-discover-meaningful-features-in-the-ioi-task.html:text/html},
}

@misc{touvron_llama_nodate,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models} {\textbar} {Research} - {AI} at {Meta}},
	url = {https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/},
	urldate = {2024-06-19},
	author = {Touvron, Hugo},
	file = {Llama 2\: Open Foundation and Fine-Tuned Chat Models | Research - AI at Meta:/home/user/Zotero/storage/A9V455ST/llama-2-open-foundation-and-fine-tuned-chat-models.html:text/html},
}

@misc{noauthor_improving_2024,
	title = {Improving {Dictionary} {Learning} with {Gated} {Sparse} {Autoencoders}},
	url = {https://deepmind.google/research/publications/88147/},
	abstract = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse,...},
	language = {en},
	urldate = {2024-06-20},
	journal = {Google DeepMind},
	month = apr,
	year = {2024},
	file = {Snapshot:/home/user/Zotero/storage/D43BZMID/88147.html:text/html},
}

@article{nanda_open_2023,
	title = {Open {Source} {Replication} \& {Commentary} on {Anthropic}'s {Dictionary} {Learning} {Paper}},
	url = {https://www.alignmentforum.org/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s},
	abstract = {This is the long-form version of a public comment on Anthropic's Towards Monosemanticity paper …},
	language = {en},
	urldate = {2024-06-24},
	author = {Nanda, Neel},
	month = oct,
	year = {2023},
	file = {Snapshot:/home/user/Zotero/storage/HADCLIQ8/open-source-replication-and-commentary-on-anthropic-s.html:text/html},
}

@inproceedings{lee_efficient_2006,
	title = {Efficient sparse coding algorithms},
	volume = {19},
	url = {https://proceedings.neurips.cc/paper_files/paper/2006/hash/2d71b2ae158c7c5912cc0bbde2bb9d95-Abstract.html},
	abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1 -regularized least squares problem and an L2 -constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.},
	urldate = {2024-06-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew},
	year = {2006},
	file = {Full Text PDF:/home/user/Zotero/storage/ZSGCLSN8/Lee et al. - 2006 - Efficient sparse coding algorithms.pdf:application/pdf},
}
