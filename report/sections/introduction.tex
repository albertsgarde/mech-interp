\documentclass[../main.tex]{subfiles}


\begin{document}

Large Language Models (LLMs) based on the transformer architecture \citep{vaswani_attention_2023} have shown exceptional performance across a range of tasks, yet how they achieve such impressive results remains poorly understood.
Though it is easy to describe what mathematical operations occur on which numbers, we have yet to understand what algorithms and concepts these calculations represent.
This opacity limits our ability to interpret, trust, and safely deploy these models in real-world applications.
The related fields of interpretability and explainability aim to address this issue by providing insights into the behaviour of these models.
Many approaches have been developed within these fields.
\citet{bereska_mechanistic_2024} provides a recent overview and a useful taxonomy, while \citet{rauker_toward_2023} provides a more thorough survey including a list of previous reviews in the same area.
Transformer models contain both attention and multi-layer perceptron (MLP) layers and the latter is the focus of this thesis.
Previous attempts at interpreting MLP neurons have focused on understanding the behaviour of individual neurons\todo{cite. N2G}, but as demonstrated in \citet{elhage_toy_2022}, the behaviour of individual neurons often doesn't map onto human understandable concepts.
\citet{bricken_towards_2023} shows a possible way forward by suggesting the features of SAEs (Sparse Autoencoders) trained on the MLP layer activations as alternative units of interpretability.
In this work, we focus on these along with the Neuron2Graph \citep{foote_neuron_2023} method.
This is an interesting combination, since if SAEs truly do provide more interpretable features and N2G truly does provide a useful representation of feature behaviour, we would expect this to be reflected when comparing N2G graphs for individual neurons against those for SAE features. 

In this work we first provide a review of the literature on SAEs as applied to interpretability of transformer models.
This review differs from the reviews mentioned above by focussing exclusively on SAEs.
This allows us to go into considerable detail, and since the field is so young, it means we can cover essentially all relevant work.
We then perform \todo{insert experiment here}.



\begin{itemize}
    \item Recently LLMs have bla bla bla
    \item Many benefits, but also worries of harm
    \begin{itemize}
        \item List a few types of harm
    \end{itemize}
    \item One issue is models are opaque. We cannot understand their behaviour to foresee or guarantee against harmful behaviour, and we have few options to correct harmful behaviours we do know about.
    \item Interpretability aims to address this issue by providing insights into the internal workings of these models.
    \item Many approaches. See \citet{bereska_mechanistic_2024} for a review
    \item We focus on...
    \item Specifically these existing methods
    \item And we do this
    \begin{itemize}
        \item Provide a review of work on sparse autoencoders
        \item Perform <insert experiment here>
    \end{itemize}
\end{itemize}

\subbib{../main}
\end{document}