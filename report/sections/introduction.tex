\documentclass[../main.tex]{subfiles}


\begin{document}

\begin{itemize}
    \item Motivation
    \begin{itemize}
        \item Large Language Models (LLMs) have shown exceptional performance across a range of tasks, yet their complexity renders their inner workings opaque.
        This opacity challenges our ability to understand, trust, and safely deploy these models in real-world applications.
        To rectify this, the overlapping fields of Explainable AI (XAI) and Mechanistic Interpretability (MI) have emerged.
        The former focuses on developing methods to explain the predictions of LLMs, while the latter focuses on understanding the inner workings of LLMs.
        Much of the focus in MI has been on understanding the behaviour of individual neurons, but as demonstrated in \citet{elhage_toy_2022}, the behaviour of individual neurons often doesn't map onto human understandable concepts.
        \citet{bricken_towards_2023} shows a possible way forward by providing alternative units of Interpretability. 
    \end{itemize}
    \item SotA
    \item Problem statement
\end{itemize}

\subbib
\end{document}